# Дипломное задание

## Этап 1. Создание облачной инфраструктуры

Подготовим облачную инфраструктуру с помощью Terraform.
В целях удобства разобьём процесс создание на несколько этапов:

- создание каталога Yandex.Cloud;
- создание бэкенда в Object Storage Yandex.Cloud для хранения файлов состояния ".tfstate";
- создание конфигурации облачной инфраструктуры.

Такое разбиение обусловлено тем, что каждый последующий этап требует полного выполнения предыдущего и использует
результаты его выполнения.

Так выполнение первого этапа позволит нам получить идентификаторы облака и каталога Yandex.Cloud, а также
сгенерировать ключи доступа, что в свою очередь, позволит на втором этапе создать бэкенд для хранения состояний
конфигурации Terraform.

И только на третьем этапе мы будем обладать всей доступной инфраструктурой для дальнейшей работы.

#### Этап 1. Создание каталога

Проверим, не устарела ли наша версия Terraform:

````bash
$ terraform version
Terraform v1.3.6
on linux_amd64
````

Версия актуальна, поэтому инициализируем конфигурацию Терраформ:

````bash
$ terraform init
````

Далее создаём рабочие пространства, которые предполагается использовать. К применению предполагаются два
рабочих пространства "stage", применяемый для разработки, и "prod", в котором функционирует инфраструктура,
используемая конечными пользователями:

````bash
$ terraform workspace new stage
...
$ terraform workspace new prod
...
$ terraform workspace select stage
Switched to workspace "stage".

$ terraform workspace show
stage
````

Как результат, имеем три рабочих пространства - два созданных ("stage" и "prod") и дефолтное рабочее пространство,
которое удалить нельзя (использовать пространство "default" мы не будем):

````bash
$ terraform workspace list
  default
  prod
* stage
````

Для наглядности вся наша конфигурация разделена не следующие файлы:

| Файл                                                                          | Назначение                                  |
|:------------------------------------------------------------------------------|:--------------------------------------------|
| [010_provider.tf](./terraform/000_folder/010_provider.tf)                     | Объявление провайдера YC                    |
| [020_folder.tf](./terraform/000_folder/020_folder.tf)                         | Создание каталога                           |
| [030_tf_service_account.tf](./terraform/000_folder/030_tf_service_account.tf) | Создание сервисных аккаунтов                |
| [040_symmetric_key.tf](./terraform/000_folder/040_symmetric_key.tf)           | Генерация ключа для шифрации Storage Bucket |
| [050_tf_backend_bucket.tf](./terraform/000_folder/050_tf_backend_bucket.tf)   | Создание Storage Bucket                     |
| [outputs.tf](./terraform/000_folder/outputs.tf)                               | Вывод значимой информации                   |
| [variables.tf](./terraform/000_folder/variables.tf)                           | Объявление переменных                       |

Все файлы находятся в единой папке и выполение команды `terraform apply` применит их все.

Применим [данную конфигурацию](./terraform/000_folder):

````bash
$ terraform apply -auto-approve
...
Apply complete! Resources: 8 added, 0 changed, 0 destroyed.

Outputs:

current-workspace-name = "stage"
remote_execution_determine = "Run environment: Local"
yc-id = "b1g8mq58h421raomnd64"
yc-folder-id = "b1gtbaen13blg91u0g0n"
yc-zone = "ru-central1"
````

Укажем идентификаторы облака и каталога в файле `./bashrc` (`~/.zshrc`) для того, чтобы явно не упоминать
в файлах конфигурации. Для этого используются специальные переменные окружения `YC_CLOUD_ID` и `YC_FOLDER_ID`:

````bash
export YC_CLOUD_ID=<Идентификатор Облака>
export YC_FOLDER_ID=<Идентификатор Каталога>
````

Помимо этих переменных окружения дополнительно используется переменная окружения `YC_TOKEN`, содержащая
[IAM-токен](https://cloud.yandex.ru/docs/iam/concepts/authorization/iam-token) для доступа к облаку.
Значение токена выдаётся пользователю после прохождения аутентификации и является чувствительными данными,
которые не следует хранить в конфигурации, а также в общедоступных местах.

Теперь нам нужно сгенерировать ключи для инициализации бэкенда, которое будет выполнено на следующем этапе.
Для этого сначала инициализируем командную строку Yandex.Cloud:

````bash
$ yc init                                                     
Welcome! This command will take you through the configuration process.
Pick desired action:
 [1] Re-initialize this profile 'default' with new settings 
 [2] Create a new profile
Please enter your numeric choice: 1
Please go to https://oauth.yandex.ru/authorize?response_type=token&client_id=1a6990aa636648e9b2ef855fa7bec2fb in order to obtain OAuth token.

Please enter OAuth token: [AQAAAAABL*********************PBEbq222c] 
You have one cloud available: 'netology-cloud' (id = b1g8mq58h421raomnd64). It is going to be used by default.
Please choose folder to use:
 [1] diploma-folder (id = b1gtbaen13blg91u0g0n)
 [2] Create a new folder
Please enter your numeric choice: 1
Your current folder has been set to 'diploma-folder' (id = b1gtbaen13blg91u0g0n).
Do you want to configure a default Compute zone? [Y/n] n
````

Когда командная строка инициалирована, её можно использовать для доступа к Yandex.Cloud.
Для начала получим список существующих сервисных аккаунтов:

````bash
$ yc iam service-account list                                 
+----------------------+--------------+
|          ID          |     NAME     |
+----------------------+--------------+
| aje8vc1lofmsq3rjo7cp | terraform-sa |
+----------------------+--------------+
````

Как видим, присутствует сгенерированный с помощью [конфигурации](./terraform/000_folder/030_tf_service_account.tf)
сервисный аккаунт под именем "terraform-sa":

````bash
$ yc iam access-key create --service-account-name terraform-sa
access_key:
  id: ajecs1ni04gtibd8ef6t
  service_account_id: aje8vc1lofmsq3rjo7cp
  created_at: "2023-01-16T14:23:33.276984019Z"
  key_id: YCAJE3L9r2w6B4g1bBkRFHQcT
secret: YCPOxqBzOU4YSCTO0fwVgYb3VFR-pN5eI3pqcRFU
````

> Значение секретного ключа (поле `secret`) повторно получить невозможно, поэтому его следует сразу сохранить
> в безопасном месте!

Так как значение ключей доступа также является чувствительными данными, то также сохраним их в переменных
окружения, объявляемых в файле `./bashrc` (`~/.zshrc`). Для этого используются специальные имена переменных
окружения `AWS_ACCESS_KEY_ID` и `AWS_SECRET_ACCESS_KEY`:

````bash
export AWS_ACCESS_KEY_ID=<Значение `key_id`>
export AWS_SECRET_ACCESS_KEY=<Значение `secret`>
````

После того, как значения переменных окружения были заданы необходимо повторно инициализовать командную оболочку,
чтобы данные переменные вступили в силу:

````bash
$ source ~/.zshrc  
````

Исходная конфигурация задана, теперь нужно создать в ней хранилище состояний Terraform.

#### Этап 2. Создание бэкенда для хранения файлов состояния ".tfstate"

Переходим в [следующую папку](./terraform/100_backend). Она содержит следующие файлы:

| Файл                                                       | Назначение                                 |
|:-----------------------------------------------------------|:-------------------------------------------|
| [110_provider.tf](./terraform/100_backend/110_provider.tf) | Объявление бэкенда в составе провайдера YC |
| [outputs.tf](./terraform/100_backend/outputs.tf)           | Вывод значимой информации                  |
| [variables.tf](./terraform/100_backend/variables.tf)       | Объявление переменных                      |

И инициализируем конфигурацию Terraform:

````bash
$ terraform init
````

Снова создаем требуемые рабочие пространства:

````bash
$ terraform workspace new prod
...
$ terraform workspace new stage
...
$ terraform workspace list
  default
  prod
* stage
````

Применяем конфигурацию:

````bash
$ terraform apply -auto-approve
````

Если бы при инициализации бэкенда было бы выбрано рабочее пространство `default`, то файл `tfstate` появился бы
непосредственно в корне бакета:

![tfstate.png](./images/tfstate.png)

Однако, в нашем случае дефолтное рабочее пространство не используется, поэтому в бакете
появиться папка с именем, заданным в параметре `workspace_key_prefix` бэкенда (если параметр не задавался,
то его значение по умолчанию -  `env:`). В этой папке будет лежать другая папка с именем текущего
рабочего пространства, а уже в ней - файл с именем, заданным в параметре `key` бэкенда.

|                                          |                                                      |                                                                      |
|:----------------------------------------:|:----------------------------------------------------:|:--------------------------------------------------------------------:|
| ![bucket_ws.png](./images/bucket_ws.png) | ![bucket_ws_stage.png](./images/bucket_ws_stage.png) | ![bucket_ws_stage_tfstate.png](./images/bucket_ws_stage_tfstate.png) |

> Формат пути к файлу `terraform.tfstate`: `workspace_key_prefix/workspace_name/key`
>
> Дефолтное значение параметра `workspace_key_prefix` - "env:".

Хранилище состояний Terraform создано, теперь можно приступить непосредственно к созданию облачной конфигурации.

#### 3. Создание конфигурации облачной инфраструктуры

Переходим в [следующую папку](./terraform/200_config). Она содержит следующие файлы:

| Файл                                                                    | Назначение                                                                                |
|:------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|
| [210_provider.tf](./terraform/200_config/210_provider.tf)               | Объявление бэкенда в составе провайдера YC,<br/>а также указание "terraform_remote_state" |
| [220_network.tf](./terraform/200_config/220_network.tf)                 | Объявление сети YC                                                                        |
| [230_private_subnets.tf](./terraform/200_config/230_private_subnets.tf) | Объявление подсетей YC                                                                    |
| [240_vms.tf](./terraform/200_config/240_vms.tf)                         | Объявление виртуальных машин YC                                                           |
| [main.tf](./terraform/200_config/modules/instance/main.tf)              | Модуль конфигурации виртуальной машины                                                    |
| [outputs.tf](./terraform/200_config/outputs.tf)                         | Вывод значимой информации                                                                 |
| [variables.tf](./terraform/200_config/variables.tf)                     | Объявление переменных                                                                     |

Инициализируем конфигурацию Терраформ:

````bash
$ terraform init
````

Благодаря указанию "terraform_remote_state" нам будут доступны рабочие пространства, созданные на предыдущих этапах,
и снова объявлять их в текущей папке конфигурации не нужно:

````bash
$ terraform workspace list
  default
  prod
* stage
````

Применяем [конфигурацию](./terraform/200_config):

````bash
$ terraform apply -auto-approve
````

Как результат имеем следующий инфраструктурный набор Yandex.Cloud:

![yc.png](./images/yc.png)

С составом из трех виртуальных машин:

![yc_vm_3.png](./images/yc_vm_3.png)

И трех подсетей, расположенных в разных зонах:

![yc_subnets.png](./images/yc_subnets.png)

Таким образом, посредством Terraform мы создали инфраструктуру, которая м.б.создана или удалена командами
`apply` и `destroy` без каких-либо дополнительных действий.

---

Терраформ использует сохраняемые состояния для отслеживания и управления ресурсам.
Здесь существует два варианта - интеграция с Terraform Cloud или использовать бэкенд, который определяет,
где именно Терраформу сохранять свои данные.
Такой подход позволяет нескольким пользователям иметь доступ к сохраняемым состояниям
и совместно работать с ресурсами.

Терраформ сохраняет конфигурацию в виде простого текста в двух файлах:

- `.terraform/terraform.tfstate` - содержит конфигурацию бэкенда для текущей рабочей директории;
- файлы планов, которые используют информацию из файла `.terraform/terraform.tfstate` во время создания плана чтобы
  гарантировать применение плана к правильному набору инфраструктуры;

При применении предварительного сохранённого плана Терраформ использует конфигурацию бэкенда, сохраненную в этом файле
вместо текущих настроект бэкенда.

Когда изменяется конфигурация бэкенда, то нужно снова выполнить `terraform init` для валидации и конфигурировании
бекенда до выполнения операций `plan`, `apply` или `state`.

После выполнения `terraform init` Терраформ локально создаст директорию `.terraform/`. Эта директория содержит последние
бэкенд конфигурации, включая все аутентификационные параметры, предоставленные посредством Terraform CLI.
Не следует сохранять директорию `.terraform/` в Git-репозитории, поскольку это может повлечь утечку чувствительных
данных.

Локальная конфигурация бэкенда отличается и полностью отделена от файла `terraform.tfstate`, хранящего данные состояния
о реальной инфраструктуре. Терраформ сохраняет файл `terraform.tfstate` в удалённом бэкенде.

При изменении бэкендов Терраформ предоставляет возможность мигрировать состояние в новый бэкенд. Это позволяет
переносить бэкенды без потери существующего состояния.

> ВАЖНО: Перед миграцией в новый бэкенд настоятельно рекомендуется вручную сохранить состояние путем копирования
> файла `terraform.tfstate`

---

#### Локальные бэкенды

Локальные бэкенды сохраняют состояние в локальной файловой системе, защищают эти состояния, используя системные API
и выполняют операции локально.

````bash
terraform {
  backend "local" {
    path = "relative/path/to/terraform.tfstate"
  }
}
````

````bash
data "terraform_remote_state" "foo" {
  backend = "local"

  config = {
    path = "${path.module}/../../terraform.tfstate"
  }
}
````

#### Для чего нужно удалённое состояние

По-умолчанию Терраформ сохраняет своё состояние локально в файле с именем `terraform.state`. При командной
работе с Терраформом использование локального файла затрудняет использование Терраформа потому что
каждый участник всегда должен использовать последнее состояние и быть уверенным, что никто больше не запускает
Терраформ в этот же момент.
При использовании же удаленного состояния Терраформ записывает данные состояния в удалённое хранилище, доступ
к которому м.б. разделён между членами команды.

#### Удалённые бэкенды

Удалённый бэкенд является уникальным среди всех остальных бэкендов Терраформа, потому что может и сохранять снимки
состояний и выполнять операции в Terraform Cloud.

Удалённый бэкенд может работать или с одним удаленным пространством имен Terraform Cloud, или с набором однообразно
именованных удалённых рабочих пространств (например, `networking-dev`, `networking-prod` и т.д.). В блоке `workspaces`
конфигурации бэкенда задаётся, какой из режимов используется:

- для использования единственного рабочего пространства Terraform Cloud следует указать в параметре `workspaces.name`
  полное имя удалённого рабочего пространства (например `networking-prod`);
- для использования набора удаленных пространств имен следует установить в параметр `workspaces.prefix` тот префикс,
  который используется во всех именах рабочих пространств.

Если предыдущее состояние существует, а соответствующие удалённые рабочие пространства отсутствуют или пусты, то
при запуске `terraform init` Терраформ создаст эти рабочие пространства и обновит удалённое состояние. Однако,
если для выполнения удалённых операций рабочему пространству требуются переменные или особая версия Терраформа,
то рекомендуется создавать удаленные пространства до запуска в них удаленных операций.

#### Использование рабочих пространств

Большинство команд Терраформа взаимодействуют только с текущим выбранным рабочим пространством. Это касается команд
по управлению ресурсами (`plan`, `apply` и `destroy`) и по управлению состояниями.

Можно создать несколько рабочих каталогов для поддержки нескольких экземпляров конфигурации с полностью
разделенными состояниями. Но Терраформ устанавливает отдельный кэш плагинов и модулей для каждого рабочего
каталога и поддержка нескольких каталогов ведет к перерасходу пропускной способности сети и дискового пространства.
Кроме того, такой подход требует выполнения дополнительных задач, вроде обновления конфигурации из СКВ для
каждого каталога в отдельности и реинициализации каждого каталога при изменении конфигурации. Рабочие же пространства
тем и удобны, что позволяют создавать различные наборы инфраструктуры из одной и той же рабочей конфигурации
и одного и того же кэша плагинов и модулей.

Типичным случаем использованием нескольких рабочих пространств является создание отдельной параллельной копии
инфраструктуры для тестирования изменений перед модификацией производственной инфраструктуры.

Недефолтные рабочие пространства часто связаны с функциональными ветками СКВ. Дефолтное рабочее пространств может
соответствовать веткам `main` или `trunk`, описывающими требуемое состояние производственной инфраструктуры. Когда
разработчик создаёт функциональную ветку для сохранения в ней некоторой модификации, то он также может создать
соответствующее рабочее пространство и разворачивать его во временной копии основной инфраструктуры. На этой
копии инфраструктуры разработчик может тестировать различные изменения без влияния на производственную инфраструктуру.
Когда изменение объединяется с дефолтным рабочим пространством, то тестовая инфраструктура и временное рабочее
пространство м.б. удалены.

#### Когда не следует использовать рабочие пространства

Т.о. рабочие пространства позволяют быстро переключаться между несколькими экземплярами **одной и той же конфигурации**
в пределах **одного бэкенда**. Но рабочие пространства не призваны решать все проблемы.

При использовании Терраформа для управления большими системами всё же следует создавать отдельные конфигурации,
соответствующие архитектурным границам внутри системы. Это позволяет командам управлять различными
системами по-отдельности. Сами по себе рабочие пространства не являются подходящим инструментом для декомпозиции
систем, потому что каждая из подсистем должна иметь собственную отдельную конфигурацию и бэкенд.

Кроме того, организации зачастую хотят иметь строгое разделение между разными инсталляциями одной и той же
инфраструктуры, обслуживающей различные стадии разработки различными внутренними командами. В этом случае
бэкенд каждой инсталляции часто использует различные учётные данные и контроль доступа. CLI рабочих пространств
в пределах рабочего каталога используют один и тот же бэкенд, поэтому рабочие пространства не являются
подходящим изоляционным механизмом для такого случая.

Альтернативом рабочим пространствам может являться использование модулей.

#### Внутреннее устройство рабочего пространства

Технически рабочие пространства являются эквивалентом переименования файла состояния. Терраформ дополняет удалённое
состояние набором защит и поддержек.
Рабочие пространства часто подразумеваются в качестве ресурсов с разделяемым доступом. Они являются приватными до
тех пор, пока используются как чисто локальное состояние или не будут переданы в СКВ.

Для локального состояния Терраформ сохраняет состояния рабочего пространства в директории с
именем `terraform.tfstate.d` С этой директорией следует работать также, как и с локальным файлом `terraform.state`.
Некоторые команды сохраняют свои файлы в СКВ, всё же рекомендуется вместо этого использовать удалённый бэкенд.

Для удалённого состояния рабочие пространства сохраняются непосредственно в сконфигурированный бэкенд.
Для гарантированного сохранения состояния во всех бэкендах имя рабочего пространства д.б. валидным сегментом URL
без какого-либо преобразования.

Терраформ сохраняет текущее рабочее пространство локально в игнорируемой директории `.terraform`.
Благодаря этому разные члены команды могут одновременно работать в разных рабочих пространствах.

#### Имена рабочих пространств

Для взаимодействия с рабочими пространствами через CLI Терраформ использует сокращенные имена без общего префикса.
Например, если префикс `networking-` то для переключения на рабочее пространство используется
команда `terraform workspace select prod`. Однако, такие удалённые операции Терраформа, как `plan` или `apply`
для Terraform Cloud будут выполняться в рабочем пространстве `networking-prod`.
По этой причине выражение terraform.workspace будет возвращать разный результат в зависимости от того,
настроено удалённое рабочее пространство на выполнение операций локально или удалённо. Например, в удаленном
рабочем пространстве `networking-prod` (префикс `networking-`), выражения дадут следующий результат:

- для локальных операций: `terraform.workspace=prod`;
- для удалённых операций: `terraform.workspace=networking-prod`;

---

> Можно инициализировать бэкенд командой:
> ````bash
> $ terraform init \
> -backend-config="endpoint=storage.yandexcloud.net" \
> -backend-config="bucket=tf-backend" \
> -backend-config="region=ru-central1-a" \
> -backend-config="key=tfstate" \
> -backend-config="access_key=$AWS_ACCESS_KEY_ID" \
> -backend-config="secret_key=$AWS_SECRET_ACCESS_KEY" \
> -backend-config="skip_region_validation=true" \
> -backend-config="skip_credentials_validation=true" \
> -backend-config="workspace_key_prefix=tf-state" \
> -reconfigure
>
> Initializing the backend...
> Do you want to copy existing state to the new backend?
>   Pre-existing state was found while migrating the previous "local" backend to the
>   newly configured "s3" backend. No existing state was found in the newly
>   configured "s3" backend. Do you want to copy this state to the new "s3"
>   backend? Enter "yes" to copy and "no" to start with an empty state.
>
>   Enter a value: yes
> ...
> ````
>
> Для последующей реконфигурации бэкенда чтобы игнорировать сохраненные конфигурации можно выполнить:
> `terraform init -reconfigure`

---

## Этап 2. Создание кластера Kubernetes

Создадим кластер Kubernetes, используя [Ansible](https://www.ansible.com/)
и применяя [Kubespray](https://kubernetes.io/docs/setup/production-environment/tools/kubespray/) развернём
его на созданных ранее трех виртуальных машинах Yandex.Cloud.

[Конфигурация Ansible](./terraform/200_config/infrastructure) состоит из следующих файлов:

| Наименование                         | Назначение |
|:-------------------------------------|:-----------|
| **Папка "infrastructure"**           |            |
| ansible.cfg                          |            |
| site.yaml                            |            |
| **Папка "inventory"**                |            |
| hosts.yaml                           |            |
| **Папка "inventory/group_vars/all"** |            |
| common.yaml                          |            |
| public_ips.yaml                      |            |
| **Папка "playbooks"**                |            |
| deploy-apps.ansible.yaml             |            |
| deploy-dashboard.ansible.yaml        |            |
| grant-permissions.ansible.yaml       |            |
| mount-cluster.ansible.yaml           |            |
| nfs-provisioner-install.ansible.yaml |            |
| post-setup.ansible.yaml              |            |
| pre-setup.ansible.yaml               |            |
| setup.ansible.yaml                   |            |
| **Папка "playbooks/k8s_cluster"**    |            |
| k8s-cluster.yml                      |            |
| k8s-net-calico.yml                   |            |
| **Папка "playbooks/templates"**      |            |
| admin-user-rbac.yaml                 |            |
| admin-user-sa.yaml                   |            |
| dashboard.yaml                       |            |
| deploy-pg.yaml                       |            |
| deployment.ansible.yaml              |            |
| empty-dir.ansible.yaml               |            |
| hosts.ansible.yaml.j2                |            |
| pg-pv.yaml                           |            |
| **Папка "playbooks/back-and-front"** |            |
| deploy-backend.yaml                  |            |
| deploy-frontend.yaml                 |            |
| ingress.yaml                         |            |
| **Папка "playbooks/debug"**          |            |
| dnsutils.ansible.yaml                |            |
| minipg.ansible.yaml                  |            |
|                                      |            |

Конфигурация доступа к кластеру сохранена в домашней папке пользователя по пути `~/.kube/config`:

![kube_folder.png](./images/kube_folder.png)

Теперь можно получить набор подов выполнив соответствующую команду `kubectl get pods`:

![kube_pods.png](./images/kube_pods.png)

Как результат, на трех виртуальных машинах Yandex.Cloud мы развернули работоспособный кластер Kubernetes и получили
конфигурацию доступа к нему в локальной папке `~/.kube`. Благодаря этому мы имеем возможность выполнения
команд `kubectl` из локального окружения. 