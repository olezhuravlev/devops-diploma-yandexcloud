# Дипломное задание

---

## Этап 1. Создание облачной инфраструктуры

Подготовим облачную инфраструктуру с помощью Terraform.
В целях удобства разобьём процесс создание на несколько этапов:

- создание каталога Yandex.Cloud;
- создание бэкенда в Object Storage Yandex.Cloud для хранения файлов состояния ".tfstate";
- создание конфигурации облачной инфраструктуры.

Такое разбиение обусловлено тем, что каждый последующий этап требует полного выполнения предыдущего и использует
результаты его выполнения.

Так выполнение первого этапа позволит нам получить идентификаторы облака и каталога Yandex.Cloud, а также
сгенерировать ключи доступа, что в свою очередь, позволит на втором этапе создать бэкенд для хранения состояний
конфигурации Terraform.

И только на третьем этапе мы будем обладать всей доступной инфраструктурой для дальнейшей работы.

#### Шаг 1. Создание каталога

Проверим, не устарела ли наша версия Terraform:

````bash
$ terraform version
Terraform v1.3.6
on linux_amd64
````

Версия актуальна, поэтому инициализируем конфигурацию Терраформ:

````bash
$ terraform init               

Initializing the backend...

Initializing provider plugins...
- Finding latest version of yandex-cloud/yandex...
- Installing yandex-cloud/yandex v0.84.0...
- Installed yandex-cloud/yandex v0.84.0 (unauthenticated)

Terraform has created a lock file .terraform.lock.hcl to record the provider
selections it made above. Include this file in your version control repository
so that Terraform can guarantee to make the same selections by default when
you run "terraform init" in the future.
...
Terraform has been successfully initialized!
````

Далее создаём рабочие пространства, которые предполагается использовать. К применению предполагаются два
рабочих пространства "stage", применяемый для разработки, и "prod", в котором функционирует инфраструктура,
используемая конечными пользователями. В наших целях мы будем использовать только "stage":

````bash
$ terraform workspace new stage
Created and switched to workspace "stage"!

You're now on a new, empty workspace. Workspaces isolate their state,
so if you run "terraform plan" Terraform will not see any existing state
for this configuration.
````

Как результат, имеем два рабочих пространства - созданное "stage" и дефолтное рабочее пространство,
которое удалить нельзя (использовать пространство "default" мы не будем):

````bash
$ terraform workspace list     
  default
* stage
````

Для наглядности вся наша конфигурация разделена не следующие файлы:

| Файл                                                                          | Назначение                                  |
|:------------------------------------------------------------------------------|:--------------------------------------------|
| [010_provider.tf](./terraform/000_folder/010_provider.tf)                     | Объявление провайдера YC                    |
| [020_folder.tf](./terraform/000_folder/020_folder.tf)                         | Создание каталога                           |
| [030_tf_service_account.tf](./terraform/000_folder/030_tf_service_account.tf) | Создание сервисных аккаунтов                |
| [040_symmetric_key.tf](./terraform/000_folder/040_symmetric_key.tf)           | Генерация ключа для шифрации Storage Bucket |
| [050_tf_backend_bucket.tf](./terraform/000_folder/050_tf_backend_bucket.tf)   | Создание Storage Bucket                     |
| [outputs.tf](./terraform/000_folder/outputs.tf)                               | Вывод значимой информации                   |
| [variables.tf](./terraform/000_folder/variables.tf)                           | Объявление переменных                       |

Все файлы находятся в единой папке и выполение команды `terraform apply` применит их все.

Применим [данную конфигурацию](./terraform/000_folder):

````bash
$ terraform apply -auto-approve
...
Apply complete! Resources: 8 added, 0 changed, 0 destroyed.

Outputs:
current-workspace-name = "stage"
yc-folder-id = "b1g9l9dm6trokqsnm68f"
yc-id = "b1g8mq58h421raomnd64"
yc-zone = "ru-central1"
````

Укажем идентификаторы облака и каталога в файле `./bashrc` (`~/.zshrc`) для того, чтобы явно не упоминать
в файлах конфигурации. Для этого используются специальные переменные окружения `YC_CLOUD_ID` и `YC_FOLDER_ID`:

````bash
export YC_CLOUD_ID=<Идентификатор Облака>
export YC_FOLDER_ID=<Идентификатор Каталога>
````

Помимо этих переменных окружения дополнительно используется переменная окружения `YC_TOKEN`, содержащая
[IAM-токен](https://cloud.yandex.ru/docs/iam/concepts/authorization/iam-token) для доступа к облаку.
Значение токена выдаётся пользователю после прохождения аутентификации и является чувствительными данными,
которые не следует хранить в конфигурации, а также в общедоступных местах.

Теперь нам нужно сгенерировать ключи для инициализации бэкенда, которое будет выполнено на следующем этапе.
Для этого сначала инициализируем командную строку Yandex.Cloud:

````bash
$ yc init
Welcome! This command will take you through the configuration process.
Pick desired action:
 [1] Re-initialize this profile 'default' with new settings 
 [2] Create a new profile
Please enter your numeric choice: 1
Please go to https://oauth.yandex.ru/authorize?response_type=token&client_id=1a6990aa636648e9b2ef855fa7bec2fb in order to obtain OAuth token.

Please enter OAuth token: [AQAAAAABL*********************PBEbq222c] 
You have one cloud available: 'netology-cloud' (id = b1g8mq58h421raomnd64). It is going to be used by default.
Please choose folder to use:
 [1] diploma-folder (id = b1g9l9dm6trokqsnm68f)
 [2] Create a new folder
Please enter your numeric choice: 1
Your current folder has been set to 'diploma-folder' (id = b1g9l9dm6trokqsnm68f).
Do you want to configure a default Compute zone? [Y/n] n
````

Когда командная строка инициалирована, её можно использовать для доступа к Yandex.Cloud.
Для начала получим список существующих сервисных аккаунтов:

````bash
$ yc iam service-account list                                 
+----------------------+--------------+
|          ID          |     NAME     |
+----------------------+--------------+
| aje2ue6uus0n119q08fq | terraform-sa |
+----------------------+--------------+
````

Как видим, присутствует сгенерированный с помощью [конфигурации](./terraform/000_folder/030_tf_service_account.tf)
сервисный аккаунт под именем "terraform-sa":

````bash
$ yc iam access-key create --service-account-name terraform-sa
access_key:
  id: ajepr7k41oq706metuc6
  service_account_id: aje2ue6uus0n119q08fq
  created_at: "2023-01-23T12:12:52.556125628Z"
  key_id: YCAJEH60L1uxgiyRmQ2DjTvUh
secret: YCMlLdnjHXZySXWWP39RuFRr_6Ns1yPI_QdySBgv
````

> Значение секретного ключа (поле `secret`) повторно получить невозможно, поэтому его следует сразу сохранить
> в безопасном месте!

Так как значение ключей доступа также является чувствительными данными, то также сохраним их в переменных
окружения, объявляемых в файле `./bashrc` (`~/.zshrc`). Для этого используются специальные имена переменных
окружения `AWS_ACCESS_KEY_ID` и `AWS_SECRET_ACCESS_KEY`:

````bash
export AWS_ACCESS_KEY_ID=<Значение `key_id`>
export AWS_SECRET_ACCESS_KEY=<Значение `secret`>
````

После того, как значения переменных окружения были заданы необходимо повторно инициализовать командную оболочку,
чтобы данные переменные вступили в силу:

````bash
$ source ~/.zshrc  
````

Исходная конфигурация задана, теперь нужно создать в ней хранилище состояний Terraform.

#### Шаг 2. Создание бэкенда для хранения файлов состояния ".tfstate"

Переходим в [следующую папку](./terraform/100_backend). Она содержит следующие файлы:

| Файл                                                       | Назначение                                 |
|:-----------------------------------------------------------|:-------------------------------------------|
| [110_provider.tf](./terraform/100_backend/110_provider.tf) | Объявление бэкенда в составе провайдера YC |
| [outputs.tf](./terraform/100_backend/outputs.tf)           | Вывод значимой информации                  |
| [variables.tf](./terraform/100_backend/variables.tf)       | Объявление переменных                      |

И инициализируем конфигурацию Terraform:

````bash
$ terraform init
````

Т.к. текущая конфигурация никак не связана с предыдущей, то нам придется снова создать требуемое рабочее пространство
"stage":

````bash
$ terraform workspace new stage
Created and switched to workspace "stage"!

You're now on a new, empty workspace. Workspaces isolate their state,
so if you run "terraform plan" Terraform will not see any existing state
for this configuration.

$ terraform workspace list     
  default
* stage
````

Применяем текущую конфигурацию:

````bash
$ terraform apply -auto-approve
````

Если бы при инициализации бэкенда было бы выбрано рабочее пространство `default`, то файл `tfstate` появился бы
непосредственно в корне бакета:

![tfstate.png](./images/tfstate.png)

Однако, в нашем случае дефолтное рабочее пространство не используется, поэтому в бакете
появиться папка с именем, заданным в параметре `workspace_key_prefix` бэкенда (если параметр не задавался,
то его значение по умолчанию -  `env:`). В этой папке будет лежать другая папка с именем текущего
рабочего пространства, а уже в ней - файл с именем, заданным в параметре `key` бэкенда.

|                                          |                                                      |                                                                      |
|:----------------------------------------:|:----------------------------------------------------:|:--------------------------------------------------------------------:|
| ![bucket_ws.png](./images/bucket_ws.png) | ![bucket_ws_stage.png](./images/bucket_ws_stage.png) | ![bucket_ws_stage_tfstate.png](./images/bucket_ws_stage_tfstate.png) |

> Формат пути к файлу `terraform.tfstate`: `workspace_key_prefix/workspace_name/key`

Хранилище состояний Terraform создано, теперь можно приступить непосредственно к созданию облачной конфигурации.

#### Шаг 3. Создание конфигурации облачной инфраструктуры

Переходим в [следующую папку](./terraform/200_config). Она содержит следующие файлы:

| Файл                                                                    | Назначение                                                                                |
|:------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|
| [210_provider.tf](./terraform/200_config/210_provider.tf)               | Объявление бэкенда в составе провайдера YC,<br/>а также указание "terraform_remote_state" |
| [220_network.tf](./terraform/200_config/220_network.tf)                 | Объявление сети YC                                                                        |
| [230_private_subnets.tf](./terraform/200_config/230_private_subnets.tf) | Объявление подсетей YC                                                                    |
| [240_vms.tf](./terraform/200_config/240_vms.tf)                         | Объявление виртуальных машин YC                                                           |
| [main.tf](./terraform/200_config/modules/instance/main.tf)              | Модуль конфигурации виртуальной машины                                                    |
| [outputs.tf](./terraform/200_config/outputs.tf)                         | Вывод значимой информации                                                                 |
| [variables.tf](./terraform/200_config/variables.tf)                     | Объявление переменных                                                                     |

Инициализируем конфигурацию Терраформ:

````bash
$ terraform init          
Initializing modules...
- vm-for-each in modules/instance

Initializing the backend...

Successfully configured the backend "s3"! Terraform will automatically
use this backend unless the backend configuration changes.

Initializing provider plugins...
- terraform.io/builtin/terraform is built in to Terraform
- Finding latest version of yandex-cloud/yandex...
- Installing yandex-cloud/yandex v0.84.0...
- Installed yandex-cloud/yandex v0.84.0 (unauthenticated)

Terraform has created a lock file .terraform.lock.hcl to record the provider
selections it made above. Include this file in your version control repository
so that Terraform can guarantee to make the same selections by default when
you run "terraform init" in the future.
...
Terraform has been successfully initialized!
````

Благодаря указанию "terraform_remote_state" нам будут доступны рабочие пространства, созданные на предыдущих этапах,
и снова объявлять их в текущей папке конфигурации не нужно:

````bash
$ terraform workspace list
* default
  stage
````

Просто переключимся на требуемое нам рабочее пространство "stage":

````bash
$ terraform workspace select stage
Switched to workspace "stage".

$ terraform workspace list        
  default
* stage
````

Применяем [конфигурацию](./terraform/200_config):

````bash
$ terraform apply -auto-approve
...
Apply complete! Resources: 7 added, 0 changed, 0 destroyed.

Outputs:
current-workspace-name = "stage"
network-id = "enp04ovvjreu8qgebo0a"
remote_execution_determine = "Run environment: Local"
vpc-network-private-id = "enp04ovvjreu8qgebo0a"
vpc-subnet-private1-id = "e9bctfrh8r42nnd4nt8c"
vpc-subnet-private1-zone = "ru-central1-a"
vpc-subnet-private2-id = "e2lmjb5ivovpd6g8833u"
vpc-subnet-private2-zone = "ru-central1-b"
vpc-subnet-private3-id = "b0c945nvee2mcn1d2juu"
vpc-subnet-private3-zone = "ru-central1-c"
````

Как результат имеем следующий инфраструктурный набор Yandex.Cloud:

![yc.png](./images/yc.png)

С составом из трех виртуальных машин:

![yc_vm_3.png](./images/yc_vm_3.png)

И трех подсетей, расположенных в разных зонах:

![yc_subnets.png](./images/yc_subnets.png)

Таким образом, посредством Terraform мы создали инфраструктуру, которая м.б.создана или удалена командами
`apply` и `destroy` без каких-либо дополнительных действий.

Теперь можно приступать к созданию кластера Kubernetes.

---

## Этап 2. Создание кластера Kubernetes

Создадим кластер Kubernetes, используя [Ansible](https://www.ansible.com/)
и применяя [Kubespray](https://kubernetes.io/docs/setup/production-environment/tools/kubespray/) развернём
его на созданных ранее трех виртуальных машинах Yandex.Cloud.

[Конфигурация Ansible](./ansible/infrastructure) состоит из следующих файлов:

| Наименование                                                                                                    | Назначение                                                           |
|:----------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------|
| [**Папка "infrastructure"**](./ansible/infrastructure)                                                          | Корневая папка Ansible                                               |
| [ansible.cfg](./ansible/infrastructure/ansible.cfg)                                                             | Параметры Ansible                                                    |
| [site.yaml](./ansible/infrastructure/site.yaml)                                                                 | Перечень файлов конфигурации для исполнения                          |
| [**Папка "inventory"**](./ansible/infrastructure/inventory)                                                     | Папка переменных                                                     |
| [hosts.yaml](./ansible/infrastructure/inventory/hosts.yaml)                                                     | Указание IP-адресов хостов                                           |
| [**Папка "inventory/group_vars/all"**](./ansible/infrastructure/inventory/group_vars/all)                       | Папка переменных, предназначенных группам хостов                     |
| [common.yaml](./ansible/infrastructure/inventory/group_vars/all/common.yaml)                                    | Переменные, не предполагаемые к изменению                            |
| [public_ips.yaml](./ansible/infrastructure/inventory/group_vars/all/public_ips.yaml)                            | Публичные IP хостов                                                  |
| [**Папка "playbooks"**](./ansible/infrastructure/playbooks)                                                     | Папка файлов конфигурации                                            |
| [deploy-dashboard.ansible.yaml](./ansible/infrastructure/playbooks/deploy-dashboard.ansible.yaml)               | Разворачивание графической панели Kubernetes                         |
| [grant-permissions.ansible.yaml](./ansible/infrastructure/playbooks/grant-permissions.ansible.yaml)             | Выдача разрешений на кластер                                         |
| [mount-cluster.ansible.yaml](./ansible/infrastructure/playbooks/mount-cluster.ansible.yaml)                     | Запуск Kubespray для разворачивания кластера                         |
| [post-setup.ansible.yaml](./ansible/infrastructure/playbooks/post-setup.ansible.yaml)                           | Удаление временных файлов                                            |
| [pre-setup.ansible.yaml](./ansible/infrastructure/playbooks/pre-setup.ansible.yaml)                             | Заполнение файла параметров кластера<br/>и передача приватного ключа |
| [setup.ansible.yaml](./ansible/infrastructure/playbooks/setup.ansible.yaml)                                     | Установка инструментов кластера                                      |
| [**Папка "playbooks/k8s_cluster"**](./ansible/infrastructure/playbooks/k8s_cluster)                             | Шаблоны параметров кластера                                          |
| [k8s-cluster.yml](./ansible/infrastructure/playbooks/k8s_cluster/k8s-cluster.yml)                               | Шаблон параметров Kubernetes                                         |
| [k8s-net-calico.yml](./ansible/infrastructure/playbooks/k8s_cluster/k8s-net-calico.yml)                         | Шаблон параметров CNI-плагина Calico                                 |
| [**Папка "playbooks/templates"**](./ansible/infrastructure/playbooks/templates)                                 | Компоненты кластера                                                  |
| [admin-user-rbac.yaml](./ansible/infrastructure/playbooks/templates/admin-user-rbac.yaml)                       | ClusterRoleBinding администратора                                    |
| [admin-user-sa.yaml](./ansible/infrastructure/playbooks/templates/admin-user-sa.yaml)                           | ServiceAccount администратора                                        |
| [dashboard.yaml](./ansible/infrastructure/playbooks/templates/dashboard.yaml)                                   | Компоненты графической панели Kubernetes                             |
| [deploy-pg.yaml](./ansible/infrastructure/playbooks/templates/deploy-pg.yaml)                                   | Разворачивание StatefulSet БД PostgreSQL                             |
| [deployment.ansible.yaml](./ansible/infrastructure/playbooks/templates/deployment.ansible.yaml)                 | Deployment экземпляра Nginx                                          |
| [empty-dir.ansible.yaml](./ansible/infrastructure/playbooks/templates/empty-dir.ansible.yaml)                   | Pod экземпляра Nginx                                                 |
| [hosts.ansible.yaml.j2](./ansible/infrastructure/playbooks/templates/hosts.ansible.yaml.j2)                     | Шаблон параметров кластера                                           |
| [**Папка "playbooks/debug"**](./ansible/infrastructure/playbooks/templates/debug)                               | Папка вспомогательных инструментов                                   |
| [dnsutils.ansible.yaml](./ansible/infrastructure/playbooks/templates/debug/dnsutils.ansible.yaml)               | Набор DNS-утилит                                                     |
| [minipg.ansible.yaml](./ansible/infrastructure/playbooks/templates/debug/minipg.ansible.yaml)                   | Клиент БД PostgreSQL                                                 |

Запустим наши виртуальные машины Yandex.Cloud, дождёмся готовности и назначения им внешних IP-адресов, после чего
укажем эти адреса в специально предназначенном для них конфигурационном файле
[public_ips.yaml](./ansible/infrastructure/inventory/group_vars/all/public_ips.yaml):

````yaml
---
public_ip:
  cp1:   158.160.33.90
  node1: 158.160.21.98
  node2: 51.250.43.112
````

> Файл может быть любым, просто мы решили хранить внешние IP-адреса именно здесь.

Теперь перейдём в папку [конфигурации Ansible](./ansible/infrastructure) и инициализуем создание кластера:

````bash
$ ansible-playbook -i inventory/hosts.yaml site.yaml
...
TASK [Gathering Facts] *****************************************************************************************************************************************************************************************************************
ok: [cp1]
ok: [node1]
ok: [node2]
...
TASK [Install common tools] ************************************************************************************************************************************************************************************************************
changed: [cp1]
changed: [node1]
changed: [node2]
...
TASK [Available nodes] *****************************************************************************************************************************************************************************************************************
ok: [localhost] => {
    "msg": [
        "NAME    STATUS   ROLES           AGE     VERSION",
        "cp1     Ready    control-plane   4m59s   v1.24.4",
        "node1   Ready    <none>          3m27s   v1.24.4",
        "node2   Ready    <none>          3m28s   v1.24.4"
    ]
}
...
TASK [Access token is] *****************************************************************************************************************************************************************************************************************
ok: [cp1] => {
    "msg": [
        "eyJhbGciOiJSUzI1NiIsImtpZCI6ImxoTi1Fb2dOdjR1QkREaXdVWDJyTWhQTkpxd1VUaEJVc21oQ0kxTGljZ2cifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNjc0MTE0MDg5LCJpYXQiOjE2NzQxMTA0ODksImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJhZG1pbi11c2VyIiwidWlkIjoiYzk1MmIyNjUtOGEyZS00YzBkLTk3NDMtNTgwOGEwYTI2MGVjIn19LCJuYmYiOjE2NzQxMTA0ODksInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlcm5ldGVzLWRhc2hib2FyZDphZG1pbi11c2VyIn0.v5p03OfBmW38y3hkuPiWC5zKaQoH9bfIEPwdtcLtvhMxd39cQV5N8cFsBP5Y2HqcT723EPLaSeDE1DaneE_ncz-uZw5eUDapAqd4SMNSK7UXltOdGEqipZQR1exk9gd2iuDYgLNZRoTrSq2Ujgg9hsjek2ixf7-Xf_282qQOZ86FGZH8W9oXSqv8pusbBIbv3yvMUyyOx_6c1RKEE4g41OQ09jH4-L0qH3JLvMmCDDOqP0hwMc4hGsu3LdrfmS8hfTis5ektXgjHcoOOXI_FbIymq6SATFQQM_LWMdVlTVlQ0ywGUcfx5beyW4A4XjyDp_EuiNbHQPueAtvihI7CQg",
        "1. Run 'kubectl proxy' on host supposed to be used for access to Kubernetes Dashboard",
        "2. Proceed to the link: http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/",
        "3. Select 'Token' option and enter the token."
    ]
}
...
PLAY RECAP *****************************************************************************************************************************************************************************************************************************
localhost                  : ok=13   changed=4    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
cp1                        : ok=35   changed=26   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node1                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
node2                      : ok=2    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
````

После инициализации кластера конфигурация доступа к нему хранится в домашней папке пользователя по
пути `~/.kube/config`:

![kube_folder.png](./images/kube_folder.png)

Доступ к кластеру можно проверить, получив, например, набор подов командой `kubectl get pods`:

![kube_pods.png](./images/kube_pods.png)

На данный момент полная инфраструктура нашего Kubernetes-кластера выглядит следующим образом:

````bash
$ kubectl get all,cm,sts,svc,deploy,sa,rs,po,pv,pvc,ep -A -o wide                                                        
NAMESPACE              NAME                                            READY   STATUS    RESTARTS        AGE     IP               NODE    NOMINATED NODE   READINESS GATES
kube-system            pod/calico-kube-controllers-7f679c5d6f-b24lt    1/1     Running   0               8m8s    10.200.104.1     node2   <none>           <none>
kube-system            pod/calico-node-9l6mn                           1/1     Running   0               8m59s   10.20.20.1       node1   <none>           <none>
kube-system            pod/calico-node-w5rng                           1/1     Running   2 (8m28s ago)   8m59s   10.30.30.1       node2   <none>           <none>
kube-system            pod/calico-node-xvzx6                           1/1     Running   2 (8m28s ago)   8m59s   10.10.10.1       cp1     <none>           <none>
kube-system            pod/coredns-5867d9544c-7hmsp                    1/1     Running   0               7m38s   10.200.166.129   node1   <none>           <none>
kube-system            pod/coredns-5867d9544c-wjxx9                    1/1     Running   0               7m48s   10.200.176.1     cp1     <none>           <none>
kube-system            pod/dns-autoscaler-59b8867c86-29w9c             1/1     Running   0               7m42s   10.200.176.2     cp1     <none>           <none>
kube-system            pod/kube-apiserver-cp1                          1/1     Running   1               11m     10.10.10.1       cp1     <none>           <none>
kube-system            pod/kube-controller-manager-cp1                 1/1     Running   2 (10m ago)     11m     10.10.10.1       cp1     <none>           <none>
kube-system            pod/kube-proxy-6pq9z                            1/1     Running   0               9m30s   10.30.30.1       node2   <none>           <none>
kube-system            pod/kube-proxy-978bh                            1/1     Running   0               9m30s   10.10.10.1       cp1     <none>           <none>
kube-system            pod/kube-proxy-zwz67                            1/1     Running   0               9m30s   10.20.20.1       node1   <none>           <none>
kube-system            pod/kube-scheduler-cp1                          1/1     Running   2 (10m ago)     11m     10.10.10.1       cp1     <none>           <none>
kube-system            pod/nginx-proxy-node1                           1/1     Running   0               8m27s   10.20.20.1       node1   <none>           <none>
kube-system            pod/nginx-proxy-node2                           1/1     Running   0               8m26s   10.30.30.1       node2   <none>           <none>
kube-system            pod/nodelocaldns-6s2qw                          1/1     Running   0               7m42s   10.30.30.1       node2   <none>           <none>
kube-system            pod/nodelocaldns-j5mzn                          1/1     Running   0               7m42s   10.20.20.1       node1   <none>           <none>
kube-system            pod/nodelocaldns-kxzbp                          1/1     Running   0               7m42s   10.10.10.1       cp1     <none>           <none>
kubernetes-dashboard   pod/dashboard-metrics-scraper-8c47d4b5d-bbwz6   1/1     Running   0               6m25s   10.200.166.130   node1   <none>           <none>
kubernetes-dashboard   pod/kubernetes-dashboard-6c75475678-44dqd       1/1     Running   0               6m26s   10.200.104.2     node2   <none>           <none>

NAMESPACE              NAME                                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE     SELECTOR
default                service/kubernetes                  ClusterIP   10.32.0.1       <none>        443/TCP                  11m     <none>
kube-system            service/coredns                     ClusterIP   10.32.0.3       <none>        53/UDP,53/TCP,9153/TCP   7m48s   k8s-app=kube-dns
kubernetes-dashboard   service/dashboard-metrics-scraper   ClusterIP   10.32.214.180   <none>        8000/TCP                 6m26s   k8s-app=dashboard-metrics-scraper
kubernetes-dashboard   service/kubernetes-dashboard        ClusterIP   10.32.78.248    <none>        443/TCP                  6m26s   k8s-app=kubernetes-dashboard

NAMESPACE     NAME                          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE     CONTAINERS    IMAGES                                          SELECTOR
kube-system   daemonset.apps/calico-node    3         3         3       3            3           kubernetes.io/os=linux   9m      calico-node   quay.io/calico/node:v3.24.5                     k8s-app=calico-node
kube-system   daemonset.apps/kube-proxy     3         3         3       3            3           kubernetes.io/os=linux   11m     kube-proxy    registry.k8s.io/kube-proxy:v1.24.4              k8s-app=kube-proxy
kube-system   daemonset.apps/nodelocaldns   3         3         3       3            3           kubernetes.io/os=linux   7m42s   node-cache    registry.k8s.io/dns/k8s-dns-node-cache:1.21.1   k8s-app=nodelocaldns

NAMESPACE              NAME                                        READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS                  IMAGES                                                            SELECTOR
kube-system            deployment.apps/calico-kube-controllers     1/1     1            1           8m9s    calico-kube-controllers     quay.io/calico/kube-controllers:v3.24.5                           k8s-app=calico-kube-controllers
kube-system            deployment.apps/coredns                     2/2     2            2           7m49s   coredns                     registry.k8s.io/coredns/coredns:v1.9.3                            k8s-app=kube-dns
kube-system            deployment.apps/dns-autoscaler              1/1     1            1           7m47s   autoscaler                  registry.k8s.io/cpa/cluster-proportional-autoscaler-amd64:1.8.5   k8s-app=dns-autoscaler
kubernetes-dashboard   deployment.apps/dashboard-metrics-scraper   1/1     1            1           6m25s   dashboard-metrics-scraper   kubernetesui/metrics-scraper:v1.0.8                               k8s-app=dashboard-metrics-scraper
kubernetes-dashboard   deployment.apps/kubernetes-dashboard        1/1     1            1           6m26s   kubernetes-dashboard        kubernetesui/dashboard:v2.6.1                                     k8s-app=kubernetes-dashboard

NAMESPACE              NAME                                                  DESIRED   CURRENT   READY   AGE     CONTAINERS                  IMAGES                                                            SELECTOR
kube-system            replicaset.apps/calico-kube-controllers-7f679c5d6f    1         1         1       8m9s    calico-kube-controllers     quay.io/calico/kube-controllers:v3.24.5                           k8s-app=calico-kube-controllers,pod-template-hash=7f679c5d6f
kube-system            replicaset.apps/coredns-5867d9544c                    2         2         2       7m49s   coredns                     registry.k8s.io/coredns/coredns:v1.9.3                            k8s-app=kube-dns,pod-template-hash=5867d9544c
kube-system            replicaset.apps/dns-autoscaler-59b8867c86             1         1         1       7m47s   autoscaler                  registry.k8s.io/cpa/cluster-proportional-autoscaler-amd64:1.8.5   k8s-app=dns-autoscaler,pod-template-hash=59b8867c86
kubernetes-dashboard   replicaset.apps/dashboard-metrics-scraper-8c47d4b5d   1         1         1       6m25s   dashboard-metrics-scraper   kubernetesui/metrics-scraper:v1.0.8                               k8s-app=dashboard-metrics-scraper,pod-template-hash=8c47d4b5d
kubernetes-dashboard   replicaset.apps/kubernetes-dashboard-6c75475678       1         1         1       6m26s   kubernetes-dashboard        kubernetesui/dashboard:v2.6.1                                     k8s-app=kubernetes-dashboard,pod-template-hash=6c75475678

NAMESPACE              NAME                                           DATA   AGE
default                configmap/kube-root-ca.crt                     1      10m
kube-node-lease        configmap/kube-root-ca.crt                     1      10m
kube-public            configmap/cluster-info                         5      11m
kube-public            configmap/kube-root-ca.crt                     1      10m
kube-system            configmap/calico-config                        2      9m2s
kube-system            configmap/coredns                              1      7m51s
kube-system            configmap/dns-autoscaler                       1      7m39s
kube-system            configmap/extension-apiserver-authentication   6      11m
kube-system            configmap/kube-proxy                           2      11m
kube-system            configmap/kube-root-ca.crt                     1      10m
kube-system            configmap/kubeadm-config                       1      11m
kube-system            configmap/kubelet-config                       1      11m
kube-system            configmap/kubernetes-services-endpoint         0      8m59s
kube-system            configmap/nodelocaldns                         1      7m44s
kubernetes-dashboard   configmap/kube-root-ca.crt                     1      6m27s
kubernetes-dashboard   configmap/kubernetes-dashboard-settings        0      6m27s

NAMESPACE              NAME                                                SECRETS   AGE
default                serviceaccount/default                              0         10m
kube-node-lease        serviceaccount/default                              0         10m
kube-public            serviceaccount/default                              0         10m
kube-system            serviceaccount/attachdetach-controller              0         10m
kube-system            serviceaccount/bootstrap-signer                     0         10m
kube-system            serviceaccount/calico-kube-controllers              0         8m9s
kube-system            serviceaccount/calico-node                          0         9m
kube-system            serviceaccount/certificate-controller               0         10m
kube-system            serviceaccount/clusterrole-aggregation-controller   0         10m
kube-system            serviceaccount/coredns                              0         7m50s
kube-system            serviceaccount/cronjob-controller                   0         10m
kube-system            serviceaccount/daemon-set-controller                0         10m
kube-system            serviceaccount/default                              0         10m
kube-system            serviceaccount/deployment-controller                0         10m
kube-system            serviceaccount/disruption-controller                0         10m
kube-system            serviceaccount/dns-autoscaler                       0         7m45s
kube-system            serviceaccount/endpoint-controller                  0         10m
kube-system            serviceaccount/endpointslice-controller             0         10m
kube-system            serviceaccount/endpointslicemirroring-controller    0         10m
kube-system            serviceaccount/ephemeral-volume-controller          0         11m
kube-system            serviceaccount/expand-controller                    0         10m
kube-system            serviceaccount/generic-garbage-collector            0         10m
kube-system            serviceaccount/horizontal-pod-autoscaler            0         10m
kube-system            serviceaccount/job-controller                       0         10m
kube-system            serviceaccount/kube-proxy                           0         11m
kube-system            serviceaccount/namespace-controller                 0         10m
kube-system            serviceaccount/node-controller                      0         11m
kube-system            serviceaccount/nodelocaldns                         0         7m44s
kube-system            serviceaccount/persistent-volume-binder             0         10m
kube-system            serviceaccount/pod-garbage-collector                0         10m
kube-system            serviceaccount/pv-protection-controller             0         10m
kube-system            serviceaccount/pvc-protection-controller            0         10m
kube-system            serviceaccount/replicaset-controller                0         10m
kube-system            serviceaccount/replication-controller               0         11m
kube-system            serviceaccount/resourcequota-controller             0         10m
kube-system            serviceaccount/root-ca-cert-publisher               0         10m
kube-system            serviceaccount/service-account-controller           0         10m
kube-system            serviceaccount/service-controller                   0         10m
kube-system            serviceaccount/statefulset-controller               0         10m
kube-system            serviceaccount/token-cleaner                        0         10m
kube-system            serviceaccount/ttl-after-finished-controller        0         11m
kube-system            serviceaccount/ttl-controller                       0         10m
kubernetes-dashboard   serviceaccount/admin-user                           0         6m22s
kubernetes-dashboard   serviceaccount/default                              0         6m27s
kubernetes-dashboard   serviceaccount/kubernetes-dashboard                 0         6m27s

NAMESPACE              NAME                                  ENDPOINTS                                                         AGE
default                endpoints/kubernetes                  10.10.10.1:6443                                                   11m
kube-system            endpoints/coredns                     10.200.166.129:53,10.200.176.1:53,10.200.166.129:53 + 3 more...   7m49s
kubernetes-dashboard   endpoints/dashboard-metrics-scraper   10.200.166.130:8000                                               6m26s
kubernetes-dashboard   endpoints/kubernetes-dashboard        10.200.104.2:8443                                                 6m27s
````

Как результат, на трех виртуальных машинах Yandex.Cloud мы развернули работоспособный кластер Kubernetes и получили
конфигурацию доступа к нему в локальной папке `~/.kube`. Благодаря этому мы имеем возможность выполнения
команд `kubectl` из локального окружения.

На следующем этапе создадим веб-приложение, предполагаемое для развёртывания в созданном кластере Kubernetes.

---

## Этап 3. Создание тестового приложения

Подготовим простейшее [тестовое веб-приложение](./diploma-repo/web-app), состоящее из
единственной [html-страницы](./diploma-repo/web-app/index.html), отображающей некоторые сведения:

````html
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html;charset=UTF-8">
    <title>DEVOPS-14</title>
    <link rel="icon" type="image/x-icon" href="favicon.png">
</head>
<body>
<h1>Diploma work DEVOPS-14</h1>
<h2>Zhuravlev O.E.</h2>
<h3>v.0.0.1</h3>
</body>
</html>
````

Также подготовим простую [конфигурацию веб-сервера](diploma-repo/nginx/nginx.conf), позволяющую выводить статические
данные (в нашем случае - страницу
[index.html](diploma-repo/web-app/index.html)):

````nginx configuration
server {
    listen 80;
    server_name  testref.com www.testref.com; # Name of virtual server (delivered in "HOST" header).

    access_log  /var/log/nginx/domains/testref.com-access.log  main; # Access logging.
    error_log   /var/log/nginx/domains/testref.com-error.log info;   # Error logging.

    location / { # Handles a certain type of client request.
        root /; #  Directory that will be used to search for a file
        index index.html;  # Defines files that will be used as an index.
        try_files $uri /index.html; # Checks whether the specified file or directory exists.
    }
}
````

Контейнеризируем данное веб-приложение с помощью Docker используя образ
веб-сервера [Nginx](https://hub.docker.com/_/nginx/):

````Dockerfile
FROM nginx
COPY web-app/index.html /usr/share/nginx/html
ADD nginx/nginx.conf /nginx/nginx.conf
````

В целях проверки работоспособности соберем образ:

````bash
$ docker build -t olezhuravlev/diploma:0.0.1 .
Sending build context to Docker daemon  56.32kB
Step 1/3 : FROM nginx
 ---> 2d389e545974
Step 2/3 : COPY web-app/index.html /usr/share/nginx/html
 ---> Using cache
 ---> 239ee511c3d7
Step 3/3 : ADD nginx/nginx.conf /nginx/nginx.conf
 ---> Using cache
 ---> 28f4b7049a10
Successfully built 28f4b7049a10
Successfully tagged olezhuravlev/diploma:0.0.1
````

Образ собран и присутствует в репозитории под тегом `olezhuravlev/diploma:0.0.1`:

````bash
$ docker image ls
REPOSITORY                TAG     IMAGE ID       CREATED          SIZE
olezhuravlev/diploma      0.0.1   28f4b7049a10   25 minutes ago   142MB
...
````

Запустим собранный образ локально:

````bash
$ docker run --network host olezhuravlev/diploma:0.0.1 
/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf
10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf
/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
/docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh
/docker-entrypoint.sh: Configuration complete; ready for start up
2023/01/17 15:42:14 [notice] 1#1: using the "epoll" event method
2023/01/17 15:42:14 [notice] 1#1: nginx/1.23.1
2023/01/17 15:42:14 [notice] 1#1: built by gcc 10.2.1 20210110 (Debian 10.2.1-6) 
2023/01/17 15:42:14 [notice] 1#1: OS: Linux 5.19.17-1-MANJARO
2023/01/17 15:42:14 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576
2023/01/17 15:42:14 [notice] 1#1: start worker processes
2023/01/17 15:42:14 [notice] 1#1: start worker process 31
2023/01/17 15:42:14 [notice] 1#1: start worker process 32
2023/01/17 15:42:14 [notice] 1#1: start worker process 33
2023/01/17 15:42:14 [notice] 1#1: start worker process 34
::1 - - [17/Jan/2023:15:42:18 +0000] "GET / HTTP/1.1" 304 0 "-" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36" "-"
````

Видим, что контейнеризированное веб-приложение доступно по `localhost` на стандартном порту `80`:

![webapp_v001.png](./images/webapp_v001.png)

Для хранения разворачиваемого веб-приложения создадим на [github.com](https://github.com) отдельный репозиторий
с именем ["diploma-repo"](https://github.com/olezhuravlev/diploma-repo):

![github_diploma_repo.png](./images/github_diploma_repo.png)

Для хранения же Docker-образа воспользуемся репозиторием [dockerhub.com](https://hub.docker.com/). Для этого
сначала залогинимся в него (учётная запись у нас уже имеется):

````bash
$ docker login                                                
Authenticating with existing credentials...
WARNING! Your password will be stored unencrypted in /home/oleg/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

Login Succeeded
````

И после успешной аутентификации отправим наш образ в репозиторий:

````bash
$ docker push olezhuravlev/diploma:0.0.1                   
The push refers to repository [docker.io/olezhuravlev/diploma]
7e46b64469bd: Pushed 
f1b7541425c2: Pushed 
36665e416ec8: Mounted from olezhuravlev/hello-world 
31192a8593ec: Mounted from olezhuravlev/hello-world 
7ee9bf58503c: Mounted from olezhuravlev/hello-world 
a064c1703bfd: Mounted from olezhuravlev/hello-world 
9388548487b1: Mounted from olezhuravlev/hello-world 
b45078e74ec9: Mounted from olezhuravlev/hello-world 
0.0.1: digest: sha256:a99757f9eb9cb6056aec64200fd35e721bf1130616357230da4ed9aa207b7486 size: 1984
````

После этого наш образ можно
[наблюдать в репозитории](https://hub.docker.com/repository/docker/olezhuravlev/diploma) через веб-интерфейс:

![dockerhub_diploma_image_v001.png](./images/dockerhub_diploma_image_v001.png)

Таким образом мы создали [git-репозиторий](https://github.com/olezhuravlev/diploma-repo),
содержащий [тестовое веб-приложение](https://github.com/olezhuravlev/diploma-repo/tree/main/web-app) и
[Dockerfile](https://github.com/olezhuravlev/diploma-repo/blob/main/Dockerfile) для его контейнеризации, а также
разместили в реестре Dockerhub
[контейнеризированный образ](https://hub.docker.com/repository/docker/olezhuravlev/diploma) этого веб-приложения.

Создав веб-приложение развернём его в кластере Kubernetes, предварительно запустив в этом же кластере систему
мониторинга.

> > _"Сервис, не имеющий системы мониторинга, должен предоставляться бесплатно"._
>>
> Цитаты великих.

---

## Этап 4. Подготовка cистемы мониторинга и разворачивание приложения в кластере Kubernetes

### Генерация манифестов развертки

Создадим систему мониторинга с помощью пакета [Kube-Prometheus](https://github.com/prometheus-operator/kube-prometheus).

Этот пакет содержит в себе полный набор инструментов, позволяющих реализовать мониторинг кластера и приложений,
работающих в нём. В состав пакета входят:

- **Prometheus Operator** - инструмент разворачивания приложений в кластере, а также контроля системы мониторинга;
- **Prometheus** - инструмент сбора метрик с серверов по HTTP-протоколу с использованием правил. Для запросов
  используется язык PromQL;
- **Alert Manager** - система оповещения для Prometheus, призванная следить за сопоставлением правил уведомлений и
  событий, находящихся в TSDB Prometheus;
- **Prometheus Node Exporter** - экспортер метрик хостовой ОС. Запускается на каждом хосте и отвечает за передачу данных
  в Prometheus;
- **Prometheus Adapter for Kubernetes Metrics APIs** - адаптер метрик;
- **kube-state-metrics** - сервис, прослушивающий сервер Kubernetes API и генерирующий метрики состояния его объектов;
- **Grafana** - инструмент визуализации данных, поставляемых различными источниками.

Пакет Kube-Prometheus позволяет кастомизацию устанавливаемых пакетов. Для этого создадим папку
[kube-prometheus](./kube-prometheus) и развернём в ней необходимые компоненты.

Потребуется установить пакет поддержки языка Golang:

````bash
$ sudo pacman -S go
````

Для удобства путь к исполняемым файлам Golang следует указать в PATH:

````bash
export PATH=$PATH:$HOME/go/bin
````

Далее следует установить инструмент [jsonnet-bundler](https://jsonnet.org/):

````bash
$ go install -a github.com/jsonnet-bundler/jsonnet-bundler/cmd/jb@latest
go: downloading github.com/jsonnet-bundler/jsonnet-bundler v0.5.1
go: downloading github.com/pkg/errors v0.9.1
go: downloading gopkg.in/alecthomas/kingpin.v2 v2.2.6
go: downloading github.com/fatih/color v1.13.0
go: downloading github.com/mattn/go-colorable v0.1.12
go: downloading github.com/mattn/go-isatty v0.0.14
go: downloading github.com/alecthomas/template v0.0.0-20190718012654-fb15b899a751
go: downloading github.com/alecthomas/units v0.0.0-20211218093645-b94a6e3cc137
go: downloading golang.org/x/sys v0.0.0-20220615213510-4f61da869c0c
````

Далее переходим в папку [kube-prometheus](./kube-prometheus), инициализуем в ней "jsonnet-bundler" и устанавливаем
необходимые для "kube-prometheus" зависимости:

````bash
$ jb init

$ jb install github.com/prometheus-operator/kube-prometheus/jsonnet/kube-prometheus@main
GET https://github.com/prometheus-operator/kube-prometheus/archive/67c08545193ebdd75f57e0afa922f87d56c98781.tar.gz 200
GET https://github.com/kubernetes/kube-state-metrics/archive/c75215203025bdbfd57bb4dfb191cc8142cfc087.tar.gz 200
GET https://github.com/prometheus/node_exporter/archive/a3bd2e13052929663dbd7d680fab4a952efb1de6.tar.gz 200
GET https://github.com/prometheus/prometheus/archive/9f91215bf6d22d1eea7b56d17a9b55ebd72dd32e.tar.gz 200
GET https://github.com/prometheus/alertmanager/archive/a8e4c166a86df10acca7612c9db462aa4e663bfe.tar.gz 200
GET https://github.com/pyrra-dev/pyrra/archive/31b9ac3c75e7aa904ea1b5fca37f6b2ea2622230.tar.gz 200
GET https://github.com/brancz/kubernetes-grafana/archive/d039275e4916aceae1c137120882e01d857787ac.tar.gz 200
GET https://github.com/etcd-io/etcd/archive/756875151fa61d040119ecce72ad909a5cce6175.tar.gz 200
GET https://github.com/prometheus-operator/prometheus-operator/archive/e5ee07bda82538d1122be100b6cd89f2a54b8e35.tar.gz 200
GET https://github.com/kubernetes-monitoring/kubernetes-mixin/archive/05ca993f01ee4dada3ea4d2afd51693784c6d858.tar.gz 200
GET https://github.com/kubernetes/kube-state-metrics/archive/c75215203025bdbfd57bb4dfb191cc8142cfc087.tar.gz 200
GET https://github.com/thanos-io/thanos/archive/57145061be15671d23493c60d8a500e70849b865.tar.gz 200
GET https://github.com/grafana/grafana/archive/1120f9e255760a3c104b57871fcb91801e934382.tar.gz 200
GET https://github.com/prometheus-operator/prometheus-operator/archive/e5ee07bda82538d1122be100b6cd89f2a54b8e35.tar.gz 200
GET https://github.com/grafana/grafonnet-lib/archive/30280196507e0fe6fa978a3e0eaca3a62844f817.tar.gz 200
GET https://github.com/grafana/jsonnet-libs/archive/a924ab1b5fd4e6eacd7235a20978d050a27bdb65.tar.gz 200
GET https://github.com/grafana/grafonnet-lib/archive/30280196507e0fe6fa978a3e0eaca3a62844f817.tar.gz 200
````

Далее скачиваем файл [example.jsonnet](./kube-prometheus/example.jsonnet) используемый в качестве образца файла
конфигурирования:

````bash
$ wget https://raw.githubusercontent.com/prometheus-operator/kube-prometheus/main/example.jsonnet -O example.jsonnet
--2023-01-19 08:12:19--  https://raw.githubusercontent.com/prometheus-operator/kube-prometheus/main/example.jsonnet
Загружен сертификат CA «/etc/ssl/certs/ca-certificates.crt»
Распознаётся raw.githubusercontent.com (raw.githubusercontent.com)… 185.199.111.133, 185.199.109.133, 185.199.110.133, ...
Подключение к raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... соединение установлено.
HTTP-запрос отправлен. Ожидание ответа… 200 OK
Длина: 2273 (2,2K) [text/plain]
Сохранение в: «example.jsonnet»

example.jsonnet    100%[=====================================================================================================================================>]   2,22K  --.-KB/s    за 0s      

2023-01-19 08:12:20 (27,6 MB/s) - «example.jsonnet» сохранён [2273/2273]
````

А также скачиваем скрипт, используемый для сборки:

````bash
$ wget https://raw.githubusercontent.com/prometheus-operator/kube-prometheus/main/build.sh -O build.sh
--2023-01-19 08:14:15--  https://raw.githubusercontent.com/prometheus-operator/kube-prometheus/main/build.sh
Загружен сертификат CA «/etc/ssl/certs/ca-certificates.crt»
Распознаётся raw.githubusercontent.com (raw.githubusercontent.com)… 185.199.108.133, 185.199.109.133, 185.199.111.133, ...
Подключение к raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... соединение установлено.
HTTP-запрос отправлен. Ожидание ответа… 200 OK
Длина: 679 [text/plain]
Сохранение в: «build.sh»

build.sh    100%[=====================================================================================================================================>]     679  --.-KB/s    за 0s      

2023-01-19 08:14:16 (34,8 MB/s) - «build.sh» сохранён [679/679]
````

Полученный файл [build.sh](./kube-prometheus/build.sh) помечаем как исполняемый:

````bash
$ chmod u+x build.sh

$ ls -la
итого 32
drwxr-xr-x 3 oleg oleg 4096 янв 19 08:14 .
drwxr-xr-x 8 oleg oleg 4096 янв 19 08:17 ..
-rwxr--r-- 1 oleg oleg  679 янв 19 08:14 build.sh
-rw-r--r-- 1 oleg oleg 2273 янв 19 08:12 example.jsonnet
-rw-r--r-- 1 oleg oleg  287 янв 19 08:10 jsonnetfile.json
-rw-r--r-- 1 oleg oleg 5287 янв 19 08:16 jsonnetfile.lock.json
drwxr-xr-x 3 oleg oleg 4096 янв 19 08:16 vendor
````

Далее обновляем зависимости для "kube-prometheus":

````bash
$ jb update                                                                             
GET https://github.com/prometheus-operator/kube-prometheus/archive/67c08545193ebdd75f57e0afa922f87d56c98781.tar.gz 200
GET https://github.com/prometheus/alertmanager/archive/a8e4c166a86df10acca7612c9db462aa4e663bfe.tar.gz 200
GET https://github.com/pyrra-dev/pyrra/archive/31b9ac3c75e7aa904ea1b5fca37f6b2ea2622230.tar.gz 200
GET https://github.com/thanos-io/thanos/archive/57145061be15671d23493c60d8a500e70849b865.tar.gz 200
GET https://github.com/brancz/kubernetes-grafana/archive/d039275e4916aceae1c137120882e01d857787ac.tar.gz 200
GET https://github.com/prometheus-operator/prometheus-operator/archive/e5ee07bda82538d1122be100b6cd89f2a54b8e35.tar.gz 200
GET https://github.com/kubernetes/kube-state-metrics/archive/c75215203025bdbfd57bb4dfb191cc8142cfc087.tar.gz 200
GET https://github.com/prometheus/node_exporter/archive/a3bd2e13052929663dbd7d680fab4a952efb1de6.tar.gz 200
GET https://github.com/kubernetes/kube-state-metrics/archive/c75215203025bdbfd57bb4dfb191cc8142cfc087.tar.gz 200
GET https://github.com/prometheus/prometheus/archive/9f91215bf6d22d1eea7b56d17a9b55ebd72dd32e.tar.gz 200
GET https://github.com/grafana/grafana/archive/1120f9e255760a3c104b57871fcb91801e934382.tar.gz 200
GET https://github.com/etcd-io/etcd/archive/756875151fa61d040119ecce72ad909a5cce6175.tar.gz 200
GET https://github.com/prometheus-operator/prometheus-operator/archive/e5ee07bda82538d1122be100b6cd89f2a54b8e35.tar.gz 200
GET https://github.com/kubernetes-monitoring/kubernetes-mixin/archive/05ca993f01ee4dada3ea4d2afd51693784c6d858.tar.gz 200
GET https://github.com/grafana/grafonnet-lib/archive/30280196507e0fe6fa978a3e0eaca3a62844f817.tar.gz 200
GET https://github.com/grafana/jsonnet-libs/archive/a924ab1b5fd4e6eacd7235a20978d050a27bdb65.tar.gz 200
GET https://github.com/grafana/grafonnet-lib/archive/30280196507e0fe6fa978a3e0eaca3a62844f817.tar.gz 200
````

Перед компиляцией следует дополнительно установить инструменты "gojsontoyaml" и "jsonnet":

````bash
$ go install github.com/brancz/gojsontoyaml@latest
go: downloading github.com/brancz/gojsontoyaml v0.1.0
go: downloading github.com/ghodss/yaml v1.0.0
go: downloading gopkg.in/yaml.v2 v2.4.0

$ go install github.com/google/go-jsonnet/cmd/jsonnet@latest
go: downloading github.com/google/go-jsonnet v0.19.1
go: downloading github.com/fatih/color v1.12.0
go: downloading sigs.k8s.io/yaml v1.1.0
go: downloading gopkg.in/yaml.v2 v2.2.7
go: downloading github.com/mattn/go-isatty v0.0.12
go: downloading github.com/mattn/go-colorable v0.1.8
go: downloading golang.org/x/sys v0.1.0
````

Создаём копию файла [example.jsonnet](./kube-prometheus/example.jsonnet) под
именем [monitoring.jsonnet](./kube-prometheus/monitoring.jsonnet)
и указываем в нем [настройки](https://github.com/prometheus-operator/kube-prometheus/tree/main/docs/customizations)
инструментов мониторинга, установка которых предполагается в кластер Kubenetes. После этого запускаем скрипт генерации
манифестов:

````bash
$ ./build.sh monitoring.jsonnet 
+ set -o pipefail
++ pwd
+ PATH=/run/media/oleg/Second/Netology/devops-diploma-yandexcloud/16-diploma-yc/kube-prometheus/tmp/bin:/opt/oracle/instantclient_21_7::/home/oleg/yandex-cloud/bin:/home/oleg/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/var/lib/flatpak/exports/bin:/usr/lib/jvm/default/bin:/usr/bin/site_perl:/usr/bin/vendor_perl:/usr/bin/core_perl:/var/lib/snapd/snap/bin:/usr/lib/jvm/amazon-corretto-11.0.16.9.1-linux-x64/bin:/usr/bin/qbec-linux-amd64:/home/oleg/go/bin
+ rm -rf manifests
+ mkdir -p manifests/setup
+ jsonnet -J vendor -m manifests monitoring.jsonnet
+ xargs '-I{}' sh -c 'cat {} | gojsontoyaml > {}.yaml' -- '{}'
+ find manifests -type f '!' -name '*.yaml' -delete
+ rm -f kustomization
````

В результате в папке [manifests](./kube-prometheus/manifests) получаем набор манифестов для разворачивания системы
мониторинга. Для их применения используется утилита `kubectl`. Её вызов производится в два этапа.


### Установка системы мониторинга

Перейдем в папку [kube-prometheus](./kube-prometheus) и создадим пространства имен и CRD (Custom Resource
Definition - [специальный ресурс](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/)
в Kubernetes, позволяющий вносить любые данные):

````bash
$ kubectl apply --server-side -f manifests/setup
namespace/monitoring serverside-applied
customresourcedefinition.apiextensions.k8s.io/alertmanagerconfigs.monitoring.coreos.com serverside-applied
customresourcedefinition.apiextensions.k8s.io/alertmanagers.monitoring.coreos.com serverside-applied
customresourcedefinition.apiextensions.k8s.io/podmonitors.monitoring.coreos.com serverside-applied
customresourcedefinition.apiextensions.k8s.io/probes.monitoring.coreos.com serverside-applied
customresourcedefinition.apiextensions.k8s.io/prometheuses.monitoring.coreos.com serverside-applied
customresourcedefinition.apiextensions.k8s.io/prometheusrules.monitoring.coreos.com serverside-applied
customresourcedefinition.apiextensions.k8s.io/servicemonitors.monitoring.coreos.com serverside-applied
customresourcedefinition.apiextensions.k8s.io/thanosrulers.monitoring.coreos.com serverside-applied
clusterrole.rbac.authorization.k8s.io/prometheus-operator serverside-applied
clusterrolebinding.rbac.authorization.k8s.io/prometheus-operator serverside-applied
deployment.apps/prometheus-operator serverside-applied
networkpolicy.networking.k8s.io/prometheus-operator serverside-applied
service/prometheus-operator serverside-applied
serviceaccount/prometheus-operator serverside-applied
````

Дождёмся запуска всех ресурсов и запустим ресурсы, непосредственно реализующие систему мониторинга:

````bash
$ kubectl apply -f manifests
alertmanager.monitoring.coreos.com/main created
networkpolicy.networking.k8s.io/alertmanager-main created
poddisruptionbudget.policy/alertmanager-main created
prometheusrule.monitoring.coreos.com/alertmanager-main-rules created
secret/alertmanager-main created
service/alertmanager-main created
serviceaccount/alertmanager-main created
servicemonitor.monitoring.coreos.com/alertmanager-main created
clusterrole.rbac.authorization.k8s.io/blackbox-exporter created
clusterrolebinding.rbac.authorization.k8s.io/blackbox-exporter created
configmap/blackbox-exporter-configuration created
deployment.apps/blackbox-exporter created
networkpolicy.networking.k8s.io/blackbox-exporter created
service/blackbox-exporter created
serviceaccount/blackbox-exporter created
servicemonitor.monitoring.coreos.com/blackbox-exporter created
secret/grafana-config created
secret/grafana-datasources created
configmap/grafana-dashboard-alertmanager-overview created
configmap/grafana-dashboard-apiserver created
configmap/grafana-dashboard-cluster-total created
configmap/grafana-dashboard-controller-manager created
configmap/grafana-dashboard-grafana-overview created
configmap/grafana-dashboard-k8s-resources-cluster created
configmap/grafana-dashboard-k8s-resources-namespace created
configmap/grafana-dashboard-k8s-resources-node created
configmap/grafana-dashboard-k8s-resources-pod created
configmap/grafana-dashboard-k8s-resources-workload created
configmap/grafana-dashboard-k8s-resources-workloads-namespace created
configmap/grafana-dashboard-kubelet created
configmap/grafana-dashboard-namespace-by-pod created
configmap/grafana-dashboard-namespace-by-workload created
configmap/grafana-dashboard-node-cluster-rsrc-use created
configmap/grafana-dashboard-node-rsrc-use created
configmap/grafana-dashboard-nodes-darwin created
configmap/grafana-dashboard-nodes created
configmap/grafana-dashboard-persistentvolumesusage created
configmap/grafana-dashboard-pod-total created
configmap/grafana-dashboard-prometheus-remote-write created
configmap/grafana-dashboard-prometheus created
configmap/grafana-dashboard-proxy created
configmap/grafana-dashboard-scheduler created
configmap/grafana-dashboard-workload-total created
configmap/grafana-dashboards created
deployment.apps/grafana created
networkpolicy.networking.k8s.io/grafana created
prometheusrule.monitoring.coreos.com/grafana-rules created
service/grafana created
serviceaccount/grafana created
servicemonitor.monitoring.coreos.com/grafana created
prometheusrule.monitoring.coreos.com/kube-prometheus-rules created
clusterrole.rbac.authorization.k8s.io/kube-state-metrics created
clusterrolebinding.rbac.authorization.k8s.io/kube-state-metrics created
deployment.apps/kube-state-metrics created
networkpolicy.networking.k8s.io/kube-state-metrics created
prometheusrule.monitoring.coreos.com/kube-state-metrics-rules created
service/kube-state-metrics created
serviceaccount/kube-state-metrics created
servicemonitor.monitoring.coreos.com/kube-state-metrics created
prometheusrule.monitoring.coreos.com/kubernetes-monitoring-rules created
servicemonitor.monitoring.coreos.com/kube-apiserver created
servicemonitor.monitoring.coreos.com/coredns created
servicemonitor.monitoring.coreos.com/kube-controller-manager created
servicemonitor.monitoring.coreos.com/kube-scheduler created
servicemonitor.monitoring.coreos.com/kubelet created
clusterrole.rbac.authorization.k8s.io/node-exporter created
clusterrolebinding.rbac.authorization.k8s.io/node-exporter created
daemonset.apps/node-exporter created
networkpolicy.networking.k8s.io/node-exporter created
prometheusrule.monitoring.coreos.com/node-exporter-rules created
service/node-exporter created
serviceaccount/node-exporter created
servicemonitor.monitoring.coreos.com/node-exporter created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
clusterrole.rbac.authorization.k8s.io/prometheus-adapter created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrolebinding.rbac.authorization.k8s.io/prometheus-adapter created
clusterrolebinding.rbac.authorization.k8s.io/resource-metrics:system:auth-delegator created
clusterrole.rbac.authorization.k8s.io/resource-metrics-server-resources created
configmap/adapter-config created
deployment.apps/prometheus-adapter created
networkpolicy.networking.k8s.io/prometheus-adapter created
poddisruptionbudget.policy/prometheus-adapter created
rolebinding.rbac.authorization.k8s.io/resource-metrics-auth-reader created
service/prometheus-adapter created
serviceaccount/prometheus-adapter created
servicemonitor.monitoring.coreos.com/prometheus-adapter created
clusterrole.rbac.authorization.k8s.io/prometheus-k8s created
clusterrolebinding.rbac.authorization.k8s.io/prometheus-k8s created
networkpolicy.networking.k8s.io/prometheus-k8s created
prometheusrule.monitoring.coreos.com/prometheus-operator-rules created
servicemonitor.monitoring.coreos.com/prometheus-operator created
poddisruptionbudget.policy/prometheus-k8s created
prometheus.monitoring.coreos.com/k8s created
prometheusrule.monitoring.coreos.com/prometheus-k8s-prometheus-rules created
rolebinding.rbac.authorization.k8s.io/prometheus-k8s-config created
rolebinding.rbac.authorization.k8s.io/prometheus-k8s created
rolebinding.rbac.authorization.k8s.io/prometheus-k8s created
rolebinding.rbac.authorization.k8s.io/prometheus-k8s created
role.rbac.authorization.k8s.io/prometheus-k8s-config created
role.rbac.authorization.k8s.io/prometheus-k8s created
role.rbac.authorization.k8s.io/prometheus-k8s created
role.rbac.authorization.k8s.io/prometheus-k8s created
service/prometheus-k8s created
serviceaccount/prometheus-k8s created
servicemonitor.monitoring.coreos.com/prometheus-k8s created
````

> Инсталляцию ресурсов в кластер можно выполнить и одной командой:
>````bash
>kubectl apply --server-side -f manifests/setup
>kubectl wait \
>  --for condition=Established \
>  --all CustomResourceDefinition \
>  --namespace=monitoring
>kubectl apply -f manifests/
>````

Когда все ресурсы запустились, можно выполнить проброску портов кластера в локальное окружение с помощью команды
`kubectl port-forward`:

````bash
$ kubectl --namespace monitoring port-forward svc/prometheus-k8s 9090
Forwarding from 127.0.0.1:9090 -> 9090
Forwarding from [::1]:9090 -> 9090
Handling connection for 9090
...
$ kubectl --namespace monitoring port-forward svc/grafana 3000
Forwarding from 127.0.0.1:3000 -> 3000
Forwarding from [::1]:3000 -> 3000
Handling connection for 3000
...
$ kubectl --namespace monitoring port-forward svc/alertmanager-main 9093
Forwarding from 127.0.0.1:9093 -> 9093
Forwarding from [::1]:9093 -> 9093
Handling connection for 9093
...
````

После чего убедиться в доступности инструментов мониторинга:

|                    Port 9090                     |                 Port 3000                  |                      Port 9093                       |
|:------------------------------------------------:|:------------------------------------------:|:----------------------------------------------------:|
| ![prometheus_on.png](./images/prometheus_on.png) | ![grafana_on.png](./images/grafana_on.png) | ![alertmanager_on.png](./images/alertmanager_on.png) |


### Разворачивание рабочей нагрузки (веб-приложения)

#### Шаблонизация веб-приложения с помощью Helm

Оснастив наш кластер системой мониторинга развернём в нём и наше веб-приложение. Для удобства разворачивания применим
инструмент [Helm](https://helm.sh/), позволяющий шаблонизировать манифесты и организовать установку приложений
из репозитория.

Создадим директорию с именем [charts](./helm), перейдём в неё и создадим в ней заготовку чартов нашего веб-приложения:

````bash
$ helm create web-app
Creating web-app
````

В папке [charts](./helm) появилась соответствующая директория [web-app](helm/web-app).

Удалим из неё те файлы и папки, которые нам не нужны:

- папка `charts` - используется для шаблонов, необходимых создаваемому шаблону; у нас таких нет;
- папка `templates/tests` - используется для хранения тестов; тестов у нас нет;
- файл `templates/hpa.yaml` - параметры горизонтального масштабирования, которое мы не используем;
- файл `templates/serviceaccount.yaml` - задаёт параметры сервисного аккаунта для доступа к API Kubernetes; нам это не
  требуется;
- файл `templates/deployment.yaml` - шаблон Deployment, который мы не используем;
- файл `templates/ingress.yaml` - шаблон Ingress, который мы не используем;
- файл `templates/service.yaml` - шаблон Service, который мы не используем.

Кроме того, из файла [_helpers.tpl](helm/web-app/templates/_helpers.tpl), являющегося локальной библиотекой
шаблонов,
удалим шаблон получения имени сервисного аккаунта `webnews-app.serviceAccountName`.

Разместим в [директории шаблонов](helm/web-app/templates) создаваемого чарта всё файлы, которые мы желаем
развернуть в нашем кластере, т.е. все файлы шаблонов нашего веб-приложения.

Ввиду простоты приложения файл у нас только один - [deploy-web-app](helm/web-app/templates/deploy-web-app.yaml),
представляющий из себя деплоймент
нашего приложения, состоящего только из фронтенд-компоненты.

Обратим внимание, что в этом файле шаблонизированы параметры репозитория, образа и его версии:

````bash
containers:
- name: diploma
  image: "{{- .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
````

Значения параметров, соответственно заданы в файле [values.yaml](helm/web-app/values.yaml):

````bash
image:
  repository: olezhuravlev/frontend
  pullPolicy: IfNotPresent
  tag: ""
````

А дефолтное значение параметра - в файле [Chart.yaml](helm/web-app/Chart.yaml):

````bash
appVersion: "0.0.1"
````

Кроме того, в файле [deploy-web-app](helm/web-app/templates/deploy-web-app.yaml) шаблонизировано применяемое
пространство имен и порт, используемый для службы NodePort:

````bash
namespace: {{ include "ns" . }}
...
nodePort: {{ include "frontend-port" . }}
````

Но здесь мы применяем собственные шаблоны, определённыйе в
файле [_helpers.tpl](helm/web-app/templates/_helpers.tpl):

````bash
{{/*
Returns name of applied namespace.
*/}}
{{- define "ns" -}}
{{- default .Release.Namespace .Values.currentNamespace }}
{{- end }}

{{/*
Returns frontend port number.
*/}}
{{- define "frontend-port" -}}
{{- "30000" }}
{{- end }}
````

Данный шаблон определен под коротким именем "ns" и буквально означает - "если переменная `currentNamespace` не
определена,
то использовать значение `default`".

Дополнительно мы определили шаблон файла [NOTES.txt](helm/web-app/templates/NOTES.txt) чтобы при запуске
нашего чарта пользователю выводилось соответствующее сообщение.


#### Разворачивание веб-приложения из чарта Helm

Шаблонизация закончена, теперь применим её и развернем наше приложение в кластере Kubernetes.

````bash
$ helm install web-app web-app
NAME: web-app
LAST DEPLOYED: Thu Jan 19 09:58:11 2023
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
Welcome to "web-app" (A Helm chart for Kubernetes) version "0.0.1" for namespace "default",
for current namespace "",
proudly build from repository "olezhuravlev/diploma".

Release revision: 1

This is installation: true
This is upgrade: false
````

Приложение развёрнуто в нашем кластере и благодаря применению NodePort доступно по адресу любой ноды кластера
через порт `30000`:

![web_app_30000_v001.png](./images/web_app_30000_v001.png)

Список приложений, развернутых с помощью "Helm", можно вывести командой:

````bash
$ helm list                   
NAME            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION      
web-app         default         1               2023-01-19 09:58:11.721129899 +0100 CET deployed        web-app-0.0.1                   0.0.1    
````

#### Размещение созданного чарта веб-приложения в репозитории Helm

Разместим созданное приложение в репозитории Helm, чтобы иметь возможность доступа к нему в любой момент.

Запаковываем репозиторий в архив:
````bash
$ helm package web-app -d charts/
Successfully packaged chart and saved it to: charts/web-app-0.0.1.tgz
````

Генерируем файл индексный файл чарта (index.html):
````bash
$ helm repo index charts/ 
````

В папке charts будет создан файл index.yaml, содержащий информацию о чарте, находящемся в указанной директории:

Кроме того, в корневой директории чарта создадим файл
[index.html](./helm/index.html), который будет являться пригласительной страницей
в наш репозиторий:
````html
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html;charset=UTF-8">
    <link rel="icon" type="image/x-icon" href="favicon.png">
</head>
<body>
<h1>Welcome to Helm repository!</h1>
<h2>Zhuravlev O.E.</h2>
</body>
</html>
````

Отправляем содержимое папки [helm](./helm) в отдельный [репозиторий](https://github.com/olezhuravlev/diploma-chart)
на GitHub:

![github_helm_repo.png](./images/github_helm_repo.png)

В настройках [репозитория](https://github.com/olezhuravlev/diploma-chart) Github переходим на закладку Pages и указываем ветку, которая будет соответствовать
нашему репозиторию. В данном случае это ветка main, выбираем её и нажимаем "Save":

![github_helm_repo_pages.png](./images/github_helm_repo_pages.png)

> Следует некоторое время подождать, пока Github создаёт страницу и генерирует URL, который можно использовать
> для создания репозитория на [artifacthub.io](https://artifacthub.io).
>
> В нашем случае это адрес [https://olezhuravlev.github.io/diploma-chart/](https://olezhuravlev.github.io/diploma-chart/)

Если перейти по [сгенерированному Github URL](https://olezhuravlev.github.io/diploma-chart/), то мы увидим пригласительную страницу нашего репозитория:

![github_helm_repo_welcome_page.png](./images/github_helm_repo_welcome_page.png)

Опубликуем наш репозиторий на хабе [artifacthub.io](https://artifacthub.io). Для этого на контрольной панели
пользователя (аккаунт у нас уже есть) выбираем закладку "Repositories" и нажимаем "Add repository":

![artifacthub_repos.png](./images/artifacthub_repos.png)

В параметрах репозитория указываем его имя и URL,  

![artifacthub_repo_add.png](./images/artifacthub_repo_add.png)

> Здесь следует обратить внимание, что в качестве URL указан адрес, предоставленный нам Github, но
> с указанием папки [charts](./helm/charts), в которой находятся
> наши файлы - индексный [index.yaml](./helm/charts/index.yaml) и [архив](./helm/charts/web-app-0.0.1.tgz)  

Репозиторий создан:

![artifacthub_repo_added.png](./images/artifacthub_repo_added.png)

#### Получение статуса "Verified Publisher"

Подтвердим нашу возможность контролировать репозиторий, чтобы получить статус "Verified Publisher".
Для это в директории, указанной в качестве URL репозитория, т.е. в [charts](./helm/charts) создадим специальный файл
метаданных [artifacthub-repo.yml](https://artifacthub.io), в котором укажем идентификатор репозитория, имя нашей учётной записи и e-mail:
````yaml
repositoryID: 1afeeefa-c585-4b84-a4b1-aeea6effd729
owners:
  - name: olezhuravlev
    email: olezhuravlev@gmail.com
````

Кроме того, чтобы сервис [artifacthub-repo.yml](https://artifacthub.io) воспринял изменение репозитория необходимо,
обновить версию приложения в репозитории.

Обновим версию приложения (это просто текст в файле [index.html](./diploma-repo/web-app/index.html)) и
пересоберем docker-образ:
````bash
$ docker build -t olezhuravlev/diploma:0.0.3 .   
Sending build context to Docker daemon  56.32kB
Step 1/3 : FROM nginx
 ---> 2d389e545974
Step 2/3 : COPY web-app/index.html /usr/share/nginx/html
 ---> d851d7f5eb01
Step 3/3 : ADD nginx/nginx.conf /nginx/nginx.conf
 ---> f406a744d842
Successfully built f406a744d842
Successfully tagged olezhuravlev/diploma:0.0.3
````

Теперь имеем несколько версий нашего приложения:
````bash
$ docker image ls                                
REPOSITORY                                TAG          IMAGE ID       CREATED              SIZE
olezhuravlev/diploma                      0.0.3        f406a744d842   About a minute ago   142MB
olezhuravlev/diploma                      0.0.2        28f4b7049a10   2 days ago           142MB
olezhuravlev/diploma                      0.0.1        28f4b7049a10   2 days ago           142MB
````

Отправим новый образ, имеющий тег `0.0.3` в репозиторий:
````bash
$ docker push olezhuravlev/diploma:0.0.3
The push refers to repository [docker.io/olezhuravlev/diploma]
7c59c9ea735a: Pushed 
4e4147fa9482: Pushed 
36665e416ec8: Layer already exists 
31192a8593ec: Layer already exists 
7ee9bf58503c: Layer already exists 
a064c1703bfd: Layer already exists 
9388548487b1: Layer already exists 
b45078e74ec9: Layer already exists 
0.0.3: digest: sha256:4e1ebff50eff8897b9517a9f9e94f315e39c156a8d71d7adf9f6d5b15d709358 size: 1984
````

Добавленный образ можно наблюдать через веб-интерфейс Dockerhub:
![dockerhub_v003.png](./images/dockerhub_v003.png)

> Если версию в Docker-репозитории не обновить, то Helm-репозиторий заботливо предупредит нас об этом, выводя в поле
> "SECURITY SCAN" предупреждение, что указанный образ не найден:
>
> ![github_helm_repo_image_not_found.png](./images/github_helm_repo_image_not_found.png)
> 
> При этом следует не просто поменять тег Docker-образа, а именно **разместить в Docker-репозитории новый образ**,
> в противном случае образ не пройдет проверку безопасности артифактория Helm!

Обновим версию приложения на `0.0.3` в файлах [values.yaml](./helm/web-app/values.yaml) и
[Chart.yaml](./helm/web-app/Chart.yaml) после чего сгенерируем новый helm-пакет (tgz-архив):

````bash
$ helm package web-app -d charts
Successfully packaged chart and saved it to: charts/web-app-0.0.3.tgz
````

Индексный файл Helm тоже нужно пересоздать:
````bash
$ helm repo index charts
````

Все изменения отправляем в тот [Git-репозиторий](https://github.com/olezhuravlev/diploma-chart/tree/main/charts),
который контролируется артифакторием Helm:

````bash
$ git commit -m "Version 0.0.3"      
[main f2129bd] Version 0.0.3
 4 files changed, 16 insertions(+), 6 deletions(-)
 create mode 100644 charts/web-app-0.0.3.tgz

$ git push   
Перечисление объектов: 14, готово.
Подсчет объектов: 100% (14/14), готово.
При сжатии изменений используется до 4 потоков
Сжатие объектов: 100% (8/8), готово.
Запись объектов: 100% (8/8), 2.52 КиБ | 860.00 КиБ/с, готово.
Всего 8 (изменений 5), повторно использовано 0 (изменений 0), повторно использовано пакетов 0
remote: Resolving deltas: 100% (5/5), completed with 5 local objects.
To github.com:olezhuravlev/diploma-chart.git
   c5b8938..f2129bd  main -> main
````

В Git-репозитории имеем новый набор файлов, в котором также присутствуют и все исторические версии пакетов:

![github_helm_repo_v003_charts.png](./images/github_helm_repo_v003_charts.png)

> Удалять старые версии не рекомендуется, потому что они уже могут быть в работе у пользователей репозитория
> и их пользовательские манифесты могут перестать собираться!

Через некоторое время (оно несколько больше, чем указано у репозитория в поле "LAST PROCESSED")
сервис [artifacthub.io](https://artifacthub.io/) посетит наш репозиторий и, обнаружив, что
файл [index.yaml](./helm/charts/index.yaml) обновлён, перечитает наш репозиторий, проверит в нём содержимое
[файла метаданных](./helm/charts/artifacthub-repo.yml) и присвоит нашему репозиторию
почётный статус "Verified Publisher":

![github_helm_repo_v003_verified.png](./images/github_helm_repo_v003_verified.png)

> Кроме присвоения статуса "Verified Publisher" наш репозиторий **успешно прошел проверку безопасности**,
> о чём свидетельствует зелёная галочка в поле "LAST SECURITY SCAN".

#### Установка приложения из Helm-репозитория

Теперь, когда наше веб-приложение добавлено в репозиторий мы можем устанавливать его в кластер автоматизированным
способом, например, через Ansible еще на этапе сборки кластере.

Создадим файл [deploy-web-app.ansible.yaml](./ansible/infrastructure/playbooks/deploy-web-app.ansible.yaml), в котором
опишем процедуру добавления нашего репозитория и установки из него веб-приложения в кластер.

Добавим указание на этот файл в файл [site.yaml](./ansible/infrastructure/site.yaml), используемый для сборки кластера,
чтобы в следующий раз получить кластер Kubernetes с уже установленным в нём веб-приложением.

Проверим работоспособность схемы установки нашего веб-приложения из репозитория "Helm".
Для этого заново установим его же, но уже из репозитория, используя созданный нами Ansible-манифест
[deploy-web-app.ansible.yaml](./ansible/infrastructure/playbooks/deploy-web-app.ansible.yaml):

````bash
$ ansible-playbook -i inventory/hosts.yaml playbooks/deploy-web-app.ansible.yaml
PLAY [Setup Kubernetes Cluster] ************************************************
TASK [Gathering Facts] *********************************************************
ok: [cp1]
TASK [Add 'Helm' chart repo] ***************************************************
ok: [cp1]
TASK [Update `Helm` repository cache] ******************************************
ok: [cp1]
TASK [Deploy certain version of web-application] *******************************
changed: [cp1]
PLAY RECAP *********************************************************************
cp1: ok=4    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
````

Как результат, имеем веб приложение, развернутое в кластере Kubernetes:
````bash
$ kubectl get deploy -n default
NAME      READY   UP-TO-DATE   AVAILABLE   AGE
web-app   2/2     2            2           7m
````

Благодаря применению NodePort приложение доступно по адресу любой ноды кластера через порт `30000` и мы видим, что это
новая версия приложения - `0.0.3`:

![web_app_30000_v003.png](./images/web_app_30000_v003.png)

> Удалять старую версию приложения не понадобилось - Kubernetes сам об этом позаботился.


### Настройка системы мониторинга

Имея работающий кластер Kubernetes и приложение в нём, создадим несложную схему мониторинга.
Допустим, мы хотим видеть общее количество подов в кластере и какая доля из них находится статусе "Running".
Аналогичные показатели мы желаем видеть для подов, относящися к нашему веб-приложению, т.е. имеющих 
идентификаторы начинающиеся на строку "web-app-".

Зайдем в интерфейс Prometheus и создадим соответствующие показатели, используя язык запросов PromQL.

PromQL-запрос для получения общего количества подов в кластере:
````bash
sum(kube_pod_status_phase{phase="Running"})
````

Доля работающих подов:
````bash
sum(kube_pod_status_phase{phase="Running"})/sum(kube_pod_status_phase)*100
````

Общее количество подов веб-приложения:
````bash
sum(kube_pod_status_phase {phase="Running", pod=~"web-app-.+"})
````

Доля работающих подов веб-приложения:
````bash
sum(kube_pod_status_phase {phase="Running", pod=~"web-app-.+"})/sum(kube_pod_status_phase {pod=~"web-app-.+"})*100
````

В веб-интерфейсе Prometheus данные PromQL-запросы выглядят следующим образом:

![prometheus_queries.png](./images/prometheus_queries.png)

Prometheus имеет определенные возможности для отображения результатов запросов, но возможности Grafana гораздо шире.
Настроим графические панели Grafana для отображения этих же параметров. Язык запросов используется тот же - PromQL,
поэтому мы можем просто скопировать в Grafana те же самые запросы, что мы создали в Prometheus.

> Если Grafana не откликается, то можно её перезапустить, удалив соответствующий под:
> ````bash$
> kubectl config set-context --current --namespace=monitoring                                                                   
> Context "kubernetes-admin@cluster.local" modified.
> 
> $ kubectl delete pod/grafana-6b4547d9b8-scv8c              
> pod "grafana-6b4547d9b8-scv8c" deleted
> ````
>
> 
Панель для отображения общего количество подов нашего кластера и количества подов, обслуживающих наше веб-приложение,
будет выглядеть следующим образом:

![grafana_panel1.png](./images/grafana_panel1.png)

Панель, отображающая долю работающих подов кластера, и долю работающих подов веб-приложения:

![grafana_panel2.png](./images/grafana_panel2.png)

Обе созданные панели можно совместить на итоговой приборной панели Grafana:

![grafana_dashboard.png](./images/grafana_dashboard.png)

Таким образом, мы имеем **систему мониторинга кластера Kubernetes и развернутого в нём веб-приложения**.

---

## Этап 5. Установка и настройка CI/CD

Развернём с помощью Терраформа еще два экземляра виртуальных машин:
- tc-server - сервер TeamCity;
- tc-agent - агент задач TeamCity;

Сервер TeamCity является более ресурсоёмким, поэтому мы назначили ему больше ресурсов - 100% времени CPU вместо 20%,
как у остальных машин, и 4Гб RAM. В то же время мы выбрали образ "Container-optimized" для запуска агента:

![yc_vm_5.png](images%2Fyc_vm_5.png)

> Список образов, оптимизированных для использования Docker, можно получить командой:
> ````bash
> $ yc compute image list --folder-id standard-images | grep "container"
> | fd80lqp9cp96q4nfae51 | container-optimized-image-1586329357 | container-optimized-image | f2enhr48fnru5k4gd30r | READY |
> | fd80m2h15krph9b36mfg | container-optimized-image-1567699175 | container-optimized-image | f2e5i3c69q4vdui0agq5 | READY |
> | fd80o2eikcn22b229tsa | container-optimized-image-v20220204a | container-optimized-image | f2elf7186figj4p34at5 | READY |
> ...
> ````
>
> Выбирать можно любой образ.


````bash
$ kubectl -n cicd port-forward svc/teamcity-server-svc 8111
Forwarding from 127.0.0.1:8111 -> 8111
Forwarding from [::1]:8111 -> 8111
Handling connection for 8111
````

````bash
$ git tag 0.0.4
````

````bash
$ git tag

0.0.4
````

````bash
$ git push --tags                       
Всего 0 (изменений 0), повторно использовано 0 (изменений 0), повторно использовано пакетов 0
To github.com:olezhuravlev/diploma-repo.git
 * [new tag]         0.0.4 -> 0.0.4
````

````bash
ubuntu@cp1:~$ sudo apt update
ubuntu@cp1:~$ sudo apt install docker.io

````

````bash
$ kubectl exec --stdin --tty pod/teamcity-agent-6899dd7b5d-pp5ml -- sh
````

````bash
100% CPU
````

````bash
````

````bash
````

````bash
````

````bash
````

````bash
````

````bash
````

````bash
````

````bash
````
