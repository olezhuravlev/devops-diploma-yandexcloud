# Дипломное задание

---

## Этап 1. Создание облачной инфраструктуры

Подготовим облачную инфраструктуру с помощью Terraform.
В целях удобства разобьём процесс создание на несколько этапов:

- создание каталога Yandex.Cloud;
- создание бэкенда в Object Storage Yandex.Cloud для хранения файлов состояния ".tfstate";
- создание конфигурации облачной инфраструктуры.

Такое разбиение обусловлено тем, что каждый последующий этап требует полного выполнения предыдущего и использует
результаты его выполнения.

Так выполнение первого этапа позволит нам получить идентификаторы облака и каталога Yandex.Cloud, а также
сгенерировать ключи доступа, что в свою очередь, позволит на втором этапе создать бэкенд для хранения состояний
конфигурации Terraform.

И только на третьем этапе мы будем обладать всей доступной инфраструктурой для дальнейшей работы.

#### Шаг 1. Создание каталога

Проверим, не устарела ли наша версия Terraform:

````bash
$ terraform version
Terraform v1.3.6
on linux_amd64
````

Версия актуальна, поэтому инициализируем конфигурацию Терраформ:

````bash
$ terraform init
````

Далее создаём рабочие пространства, которые предполагается использовать. К применению предполагаются два
рабочих пространства "stage", применяемый для разработки, и "prod", в котором функционирует инфраструктура,
используемая конечными пользователями:

````bash
$ terraform workspace new stage
...
$ terraform workspace new prod
...
$ terraform workspace select stage
Switched to workspace "stage".

$ terraform workspace show
stage
````

Как результат, имеем три рабочих пространства - два созданных ("stage" и "prod") и дефолтное рабочее пространство,
которое удалить нельзя (использовать пространство "default" мы не будем):

````bash
$ terraform workspace list
  default
  prod
* stage
````

Для наглядности вся наша конфигурация разделена не следующие файлы:

| Файл                                                                          | Назначение                                  |
|:------------------------------------------------------------------------------|:--------------------------------------------|
| [010_provider.tf](./terraform/000_folder/010_provider.tf)                     | Объявление провайдера YC                    |
| [020_folder.tf](./terraform/000_folder/020_folder.tf)                         | Создание каталога                           |
| [030_tf_service_account.tf](./terraform/000_folder/030_tf_service_account.tf) | Создание сервисных аккаунтов                |
| [040_symmetric_key.tf](./terraform/000_folder/040_symmetric_key.tf)           | Генерация ключа для шифрации Storage Bucket |
| [050_tf_backend_bucket.tf](./terraform/000_folder/050_tf_backend_bucket.tf)   | Создание Storage Bucket                     |
| [outputs.tf](./terraform/000_folder/outputs.tf)                               | Вывод значимой информации                   |
| [variables.tf](./terraform/000_folder/variables.tf)                           | Объявление переменных                       |

Все файлы находятся в единой папке и выполение команды `terraform apply` применит их все.

Применим [данную конфигурацию](./terraform/000_folder):

````bash
$ terraform apply -auto-approve
...
Apply complete! Resources: 8 added, 0 changed, 0 destroyed.

Outputs:

current-workspace-name = "stage"
remote_execution_determine = "Run environment: Local"
yc-id = "b1g8mq58h421raomnd64"
yc-folder-id = "b1gtbaen13blg91u0g0n"
yc-zone = "ru-central1"
````

Укажем идентификаторы облака и каталога в файле `./bashrc` (`~/.zshrc`) для того, чтобы явно не упоминать
в файлах конфигурации. Для этого используются специальные переменные окружения `YC_CLOUD_ID` и `YC_FOLDER_ID`:

````bash
export YC_CLOUD_ID=<Идентификатор Облака>
export YC_FOLDER_ID=<Идентификатор Каталога>
````

Помимо этих переменных окружения дополнительно используется переменная окружения `YC_TOKEN`, содержащая
[IAM-токен](https://cloud.yandex.ru/docs/iam/concepts/authorization/iam-token) для доступа к облаку.
Значение токена выдаётся пользователю после прохождения аутентификации и является чувствительными данными,
которые не следует хранить в конфигурации, а также в общедоступных местах.

Теперь нам нужно сгенерировать ключи для инициализации бэкенда, которое будет выполнено на следующем этапе.
Для этого сначала инициализируем командную строку Yandex.Cloud:

````bash
$ yc init                                                     
Welcome! This command will take you through the configuration process.
Pick desired action:
 [1] Re-initialize this profile 'default' with new settings 
 [2] Create a new profile
Please enter your numeric choice: 1
Please go to https://oauth.yandex.ru/authorize?response_type=token&client_id=1a6990aa636648e9b2ef855fa7bec2fb in order to obtain OAuth token.

Please enter OAuth token: [AQAAAAABL*********************PBEbq222c] 
You have one cloud available: 'netology-cloud' (id = b1g8mq58h421raomnd64). It is going to be used by default.
Please choose folder to use:
 [1] diploma-folder (id = b1gtbaen13blg91u0g0n)
 [2] Create a new folder
Please enter your numeric choice: 1
Your current folder has been set to 'diploma-folder' (id = b1gtbaen13blg91u0g0n).
Do you want to configure a default Compute zone? [Y/n] n
````

Когда командная строка инициалирована, её можно использовать для доступа к Yandex.Cloud.
Для начала получим список существующих сервисных аккаунтов:

````bash
$ yc iam service-account list                                 
+----------------------+--------------+
|          ID          |     NAME     |
+----------------------+--------------+
| aje8vc1lofmsq3rjo7cp | terraform-sa |
+----------------------+--------------+
````

Как видим, присутствует сгенерированный с помощью [конфигурации](./terraform/000_folder/030_tf_service_account.tf)
сервисный аккаунт под именем "terraform-sa":

````bash
$ yc iam access-key create --service-account-name terraform-sa
access_key:
  id: ajecs1ni04gtibd8ef6t
  service_account_id: aje8vc1lofmsq3rjo7cp
  created_at: "2023-01-16T14:23:33.276984019Z"
  key_id: YCAJE3L9r2w6B4g1bBkRFHQcT
secret: YCPOxqBzOU4YSCTO0fwVgYb3VFR-pN5eI3pqcRFU
````

> Значение секретного ключа (поле `secret`) повторно получить невозможно, поэтому его следует сразу сохранить
> в безопасном месте!

Так как значение ключей доступа также является чувствительными данными, то также сохраним их в переменных
окружения, объявляемых в файле `./bashrc` (`~/.zshrc`). Для этого используются специальные имена переменных
окружения `AWS_ACCESS_KEY_ID` и `AWS_SECRET_ACCESS_KEY`:

````bash
export AWS_ACCESS_KEY_ID=<Значение `key_id`>
export AWS_SECRET_ACCESS_KEY=<Значение `secret`>
````

После того, как значения переменных окружения были заданы необходимо повторно инициализовать командную оболочку,
чтобы данные переменные вступили в силу:

````bash
$ source ~/.zshrc  
````

Исходная конфигурация задана, теперь нужно создать в ней хранилище состояний Terraform.

#### Шаг 2. Создание бэкенда для хранения файлов состояния ".tfstate"

Переходим в [следующую папку](./terraform/100_backend). Она содержит следующие файлы:

| Файл                                                       | Назначение                                 |
|:-----------------------------------------------------------|:-------------------------------------------|
| [110_provider.tf](./terraform/100_backend/110_provider.tf) | Объявление бэкенда в составе провайдера YC |
| [outputs.tf](./terraform/100_backend/outputs.tf)           | Вывод значимой информации                  |
| [variables.tf](./terraform/100_backend/variables.tf)       | Объявление переменных                      |

И инициализируем конфигурацию Terraform:

````bash
$ terraform init
````

Снова создаем требуемые рабочие пространства:

````bash
$ terraform workspace new prod
...
$ terraform workspace new stage
...
$ terraform workspace list
  default
  prod
* stage
````

Применяем конфигурацию:

````bash
$ terraform apply -auto-approve
````

Если бы при инициализации бэкенда было бы выбрано рабочее пространство `default`, то файл `tfstate` появился бы
непосредственно в корне бакета:

![tfstate.png](./images/tfstate.png)

Однако, в нашем случае дефолтное рабочее пространство не используется, поэтому в бакете
появиться папка с именем, заданным в параметре `workspace_key_prefix` бэкенда (если параметр не задавался,
то его значение по умолчанию -  `env:`). В этой папке будет лежать другая папка с именем текущего
рабочего пространства, а уже в ней - файл с именем, заданным в параметре `key` бэкенда.

|                                          |                                                      |                                                                      |
|:----------------------------------------:|:----------------------------------------------------:|:--------------------------------------------------------------------:|
| ![bucket_ws.png](./images/bucket_ws.png) | ![bucket_ws_stage.png](./images/bucket_ws_stage.png) | ![bucket_ws_stage_tfstate.png](./images/bucket_ws_stage_tfstate.png) |

> Формат пути к файлу `terraform.tfstate`: `workspace_key_prefix/workspace_name/key`
>
> Дефолтное значение параметра `workspace_key_prefix` - "env:".

Хранилище состояний Terraform создано, теперь можно приступить непосредственно к созданию облачной конфигурации.

#### Шаг 3. Создание конфигурации облачной инфраструктуры

Переходим в [следующую папку](./terraform/200_config). Она содержит следующие файлы:

| Файл                                                                    | Назначение                                                                                |
|:------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|
| [210_provider.tf](./terraform/200_config/210_provider.tf)               | Объявление бэкенда в составе провайдера YC,<br/>а также указание "terraform_remote_state" |
| [220_network.tf](./terraform/200_config/220_network.tf)                 | Объявление сети YC                                                                        |
| [230_private_subnets.tf](./terraform/200_config/230_private_subnets.tf) | Объявление подсетей YC                                                                    |
| [240_vms.tf](./terraform/200_config/240_vms.tf)                         | Объявление виртуальных машин YC                                                           |
| [main.tf](./terraform/200_config/modules/instance/main.tf)              | Модуль конфигурации виртуальной машины                                                    |
| [outputs.tf](./terraform/200_config/outputs.tf)                         | Вывод значимой информации                                                                 |
| [variables.tf](./terraform/200_config/variables.tf)                     | Объявление переменных                                                                     |

Инициализируем конфигурацию Терраформ:

````bash
$ terraform init
````

Благодаря указанию "terraform_remote_state" нам будут доступны рабочие пространства, созданные на предыдущих этапах,
и снова объявлять их в текущей папке конфигурации не нужно:

````bash
$ terraform workspace list
  default
  prod
* stage
````

Применяем [конфигурацию](./terraform/200_config):

````bash
$ terraform apply -auto-approve
````

Как результат имеем следующий инфраструктурный набор Yandex.Cloud:

![yc.png](./images/yc.png)

С составом из трех виртуальных машин:

![yc_vm_3.png](./images/yc_vm_3.png)

И трех подсетей, расположенных в разных зонах:

![yc_subnets.png](./images/yc_subnets.png)

Таким образом, посредством Terraform мы создали инфраструктуру, которая м.б.создана или удалена командами
`apply` и `destroy` без каких-либо дополнительных действий.

---

Терраформ использует сохраняемые состояния для отслеживания и управления ресурсам.
Здесь существует два варианта - интеграция с Terraform Cloud или использовать бэкенд, который определяет,
где именно Терраформу сохранять свои данные.
Такой подход позволяет нескольким пользователям иметь доступ к сохраняемым состояниям
и совместно работать с ресурсами.

Терраформ сохраняет конфигурацию в виде простого текста в двух файлах:

- `.terraform/terraform.tfstate` - содержит конфигурацию бэкенда для текущей рабочей директории;
- файлы планов, которые используют информацию из файла `.terraform/terraform.tfstate` во время создания плана чтобы
  гарантировать применение плана к правильному набору инфраструктуры;

При применении предварительного сохранённого плана Терраформ использует конфигурацию бэкенда, сохраненную в этом файле
вместо текущих настроект бэкенда.

Когда изменяется конфигурация бэкенда, то нужно снова выполнить `terraform init` для валидации и конфигурировании
бекенда до выполнения операций `plan`, `apply` или `state`.

После выполнения `terraform init` Терраформ локально создаст директорию `.terraform/`. Эта директория содержит последние
бэкенд конфигурации, включая все аутентификационные параметры, предоставленные посредством Terraform CLI.
Не следует сохранять директорию `.terraform/` в Git-репозитории, поскольку это может повлечь утечку чувствительных
данных.

Локальная конфигурация бэкенда отличается и полностью отделена от файла `terraform.tfstate`, хранящего данные состояния
о реальной инфраструктуре. Терраформ сохраняет файл `terraform.tfstate` в удалённом бэкенде.

При изменении бэкендов Терраформ предоставляет возможность мигрировать состояние в новый бэкенд. Это позволяет
переносить бэкенды без потери существующего состояния.

> ВАЖНО: Перед миграцией в новый бэкенд настоятельно рекомендуется вручную сохранить состояние путем копирования
> файла `terraform.tfstate`

---

#### Локальные бэкенды

Локальные бэкенды сохраняют состояние в локальной файловой системе, защищают эти состояния, используя системные API
и выполняют операции локально.

````bash
terraform {
  backend "local" {
    path = "relative/path/to/terraform.tfstate"
  }
}
````

````bash
data "terraform_remote_state" "foo" {
  backend = "local"

  config = {
    path = "${path.module}/../../terraform.tfstate"
  }
}
````

#### Для чего нужно удалённое состояние

По-умолчанию Терраформ сохраняет своё состояние локально в файле с именем `terraform.state`. При командной
работе с Терраформом использование локального файла затрудняет использование Терраформа потому что
каждый участник всегда должен использовать последнее состояние и быть уверенным, что никто больше не запускает
Терраформ в этот же момент.
При использовании же удаленного состояния Терраформ записывает данные состояния в удалённое хранилище, доступ
к которому м.б. разделён между членами команды.

#### Удалённые бэкенды

Удалённый бэкенд является уникальным среди всех остальных бэкендов Терраформа, потому что может и сохранять снимки
состояний и выполнять операции в Terraform Cloud.

Удалённый бэкенд может работать или с одним удаленным пространством имен Terraform Cloud, или с набором однообразно
именованных удалённых рабочих пространств (например, `networking-dev`, `networking-prod` и т.д.). В блоке `workspaces`
конфигурации бэкенда задаётся, какой из режимов используется:

- для использования единственного рабочего пространства Terraform Cloud следует указать в параметре `workspaces.name`
  полное имя удалённого рабочего пространства (например `networking-prod`);
- для использования набора удаленных пространств имен следует установить в параметр `workspaces.prefix` тот префикс,
  который используется во всех именах рабочих пространств.

Если предыдущее состояние существует, а соответствующие удалённые рабочие пространства отсутствуют или пусты, то
при запуске `terraform init` Терраформ создаст эти рабочие пространства и обновит удалённое состояние. Однако,
если для выполнения удалённых операций рабочему пространству требуются переменные или особая версия Терраформа,
то рекомендуется создавать удаленные пространства до запуска в них удаленных операций.

#### Использование рабочих пространств

Большинство команд Терраформа взаимодействуют только с текущим выбранным рабочим пространством. Это касается команд
по управлению ресурсами (`plan`, `apply` и `destroy`) и по управлению состояниями.

Можно создать несколько рабочих каталогов для поддержки нескольких экземпляров конфигурации с полностью
разделенными состояниями. Но Терраформ устанавливает отдельный кэш плагинов и модулей для каждого рабочего
каталога и поддержка нескольких каталогов ведет к перерасходу пропускной способности сети и дискового пространства.
Кроме того, такой подход требует выполнения дополнительных задач, вроде обновления конфигурации из СКВ для
каждого каталога в отдельности и реинициализации каждого каталога при изменении конфигурации. Рабочие же пространства
тем и удобны, что позволяют создавать различные наборы инфраструктуры из одной и той же рабочей конфигурации
и одного и того же кэша плагинов и модулей.

Типичным случаем использованием нескольких рабочих пространств является создание отдельной параллельной копии
инфраструктуры для тестирования изменений перед модификацией производственной инфраструктуры.

Недефолтные рабочие пространства часто связаны с функциональными ветками СКВ. Дефолтное рабочее пространств может
соответствовать веткам `main` или `trunk`, описывающими требуемое состояние производственной инфраструктуры. Когда
разработчик создаёт функциональную ветку для сохранения в ней некоторой модификации, то он также может создать
соответствующее рабочее пространство и разворачивать его во временной копии основной инфраструктуры. На этой
копии инфраструктуры разработчик может тестировать различные изменения без влияния на производственную инфраструктуру.
Когда изменение объединяется с дефолтным рабочим пространством, то тестовая инфраструктура и временное рабочее
пространство м.б. удалены.

#### Когда не следует использовать рабочие пространства

Т.о. рабочие пространства позволяют быстро переключаться между несколькими экземплярами **одной и той же конфигурации**
в пределах **одного бэкенда**. Но рабочие пространства не призваны решать все проблемы.

При использовании Терраформа для управления большими системами всё же следует создавать отдельные конфигурации,
соответствующие архитектурным границам внутри системы. Это позволяет командам управлять различными
системами по-отдельности. Сами по себе рабочие пространства не являются подходящим инструментом для декомпозиции
систем, потому что каждая из подсистем должна иметь собственную отдельную конфигурацию и бэкенд.

Кроме того, организации зачастую хотят иметь строгое разделение между разными инсталляциями одной и той же
инфраструктуры, обслуживающей различные стадии разработки различными внутренними командами. В этом случае
бэкенд каждой инсталляции часто использует различные учётные данные и контроль доступа. CLI рабочих пространств
в пределах рабочего каталога используют один и тот же бэкенд, поэтому рабочие пространства не являются
подходящим изоляционным механизмом для такого случая.

Альтернативом рабочим пространствам может являться использование модулей.

#### Внутреннее устройство рабочего пространства

Технически рабочие пространства являются эквивалентом переименования файла состояния. Терраформ дополняет удалённое
состояние набором защит и поддержек.
Рабочие пространства часто подразумеваются в качестве ресурсов с разделяемым доступом. Они являются приватными до
тех пор, пока используются как чисто локальное состояние или не будут переданы в СКВ.

Для локального состояния Терраформ сохраняет состояния рабочего пространства в директории с
именем `terraform.tfstate.d` С этой директорией следует работать также, как и с локальным файлом `terraform.state`.
Некоторые команды сохраняют свои файлы в СКВ, всё же рекомендуется вместо этого использовать удалённый бэкенд.

Для удалённого состояния рабочие пространства сохраняются непосредственно в сконфигурированный бэкенд.
Для гарантированного сохранения состояния во всех бэкендах имя рабочего пространства д.б. валидным сегментом URL
без какого-либо преобразования.

Терраформ сохраняет текущее рабочее пространство локально в игнорируемой директории `.terraform`.
Благодаря этому разные члены команды могут одновременно работать в разных рабочих пространствах.

#### Имена рабочих пространств

Для взаимодействия с рабочими пространствами через CLI Терраформ использует сокращенные имена без общего префикса.
Например, если префикс `networking-` то для переключения на рабочее пространство используется
команда `terraform workspace select prod`. Однако, такие удалённые операции Терраформа, как `plan` или `apply`
для Terraform Cloud будут выполняться в рабочем пространстве `networking-prod`.
По этой причине выражение terraform.workspace будет возвращать разный результат в зависимости от того,
настроено удалённое рабочее пространство на выполнение операций локально или удалённо. Например, в удаленном
рабочем пространстве `networking-prod` (префикс `networking-`), выражения дадут следующий результат:

- для локальных операций: `terraform.workspace=prod`;
- для удалённых операций: `terraform.workspace=networking-prod`;

---

> Можно инициализировать бэкенд командой:
> ````bash
> $ terraform init \
> -backend-config="endpoint=storage.yandexcloud.net" \
> -backend-config="bucket=tf-backend" \
> -backend-config="region=ru-central1-a" \
> -backend-config="key=tfstate" \
> -backend-config="access_key=$AWS_ACCESS_KEY_ID" \
> -backend-config="secret_key=$AWS_SECRET_ACCESS_KEY" \
> -backend-config="skip_region_validation=true" \
> -backend-config="skip_credentials_validation=true" \
> -backend-config="workspace_key_prefix=tf-state" \
> -reconfigure
>
> Initializing the backend...
> Do you want to copy existing state to the new backend?
>   Pre-existing state was found while migrating the previous "local" backend to the
>   newly configured "s3" backend. No existing state was found in the newly
>   configured "s3" backend. Do you want to copy this state to the new "s3"
>   backend? Enter "yes" to copy and "no" to start with an empty state.
>
>   Enter a value: yes
> ...
> ````
>
> Для последующей реконфигурации бэкенда чтобы игнорировать сохраненные конфигурации можно выполнить:
> `terraform init -reconfigure`

---

## Этап 2. Создание кластера Kubernetes

Создадим кластер Kubernetes, используя [Ansible](https://www.ansible.com/)
и применяя [Kubespray](https://kubernetes.io/docs/setup/production-environment/tools/kubespray/) развернём
его на созданных ранее трех виртуальных машинах Yandex.Cloud.

[Конфигурация Ansible](./ansible/infrastructure) состоит из следующих файлов:

| Наименование                                                                                                    | Назначение                                                           |
|:----------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------|
| [**Папка "infrastructure"**](./ansible/infrastructure)                                                          | Корневая папка Ansible                                               |
| [ansible.cfg](./ansible/infrastructure/ansible.cfg)                                                             | Параметры Ansible                                                    |
| [site.yaml](./ansible/infrastructure/site.yaml)                                                                 | Перечень файлов конфигурации для исполнения                          |
| [**Папка "inventory"**](./ansible/infrastructure/inventory)                                                     | Папка переменных                                                     |
| [hosts.yaml](./ansible/infrastructure/inventory/hosts.yaml)                                                     | Указание IP-адресов хостов                                           |
| [**Папка "inventory/group_vars/all"**](./ansible/infrastructure/inventory/group_vars/all)                       | Папка переменных, предназначенных группам хостов                     |
| [common.yaml](./ansible/infrastructure/inventory/group_vars/all/common.yaml)                                    | Переменные, не предполагаемые к изменению                            |
| [public_ips.yaml](./ansible/infrastructure/inventory/group_vars/all/public_ips.yaml)                            | Публичные IP хостов                                                  |
| [**Папка "playbooks"**](./ansible/infrastructure/playbooks)                                                     | Папка файлов конфигурации                                            |
| [deploy-apps.ansible.yaml](./ansible/infrastructure/playbooks/deploy-apps.ansible.yaml)                         | Загрузка инструментов для разворачивания кластера Kubernetes         |
| [deploy-dashboard.ansible.yaml](./ansible/infrastructure/playbooks/deploy-dashboard.ansible.yaml)               | Разворачивание графической панели Kubernetes                         |
| [grant-permissions.ansible.yaml](./ansible/infrastructure/playbooks/grant-permissions.ansible.yaml)             | Выдача разрешений на кластер                                         |
| [mount-cluster.ansible.yaml](./ansible/infrastructure/playbooks/mount-cluster.ansible.yaml)                     | Запуск Kubespray для разворачивания кластера                         |
| [nfs-provisioner-install.ansible.yaml](./ansible/infrastructure/playbooks/nfs-provisioner-install.ansible.yaml) | Разворачивание "nfs-server-provisioner"                              |
| [post-setup.ansible.yaml](./ansible/infrastructure/playbooks/post-setup.ansible.yaml)                           | Удаление временных файлов                                            |
| [pre-setup.ansible.yaml](./ansible/infrastructure/playbooks/pre-setup.ansible.yaml)                             | Заполнение файла параметров кластера<br/>и передача приватного ключа |
| [setup.ansible.yaml](./ansible/infrastructure/playbooks/setup.ansible.yaml)                                     | Установка инструментов кластера                                      |
| [**Папка "playbooks/k8s_cluster"**](./ansible/infrastructure/playbooks/k8s_cluster)                             | Шаблоны параметров кластера                                          |
| [k8s-cluster.yml](./ansible/infrastructure/playbooks/k8s_cluster/k8s-cluster.yml)                               | Шаблон параметров Kubernetes                                         |
| [k8s-net-calico.yml](./ansible/infrastructure/playbooks/k8s_cluster/k8s-net-calico.yml)                         | Шаблон параметров Calico                                             |
| [**Папка "playbooks/templates"**](./ansible/infrastructure/playbooks/templates)                                 | Компоненты кластера                                                  |
| [admin-user-rbac.yaml](./ansible/infrastructure/playbooks/templates/admin-user-rbac.yaml)                       | ClusterRoleBinding администратора                                    |
| [admin-user-sa.yaml](./ansible/infrastructure/playbooks/templates/admin-user-sa.yaml)                           | ServiceAccount администратора                                        |
| [dashboard.yaml](./ansible/infrastructure/playbooks/templates/dashboard.yaml)                                   | Компоненты графической панели Kubernetes                             |
| [deploy-pg.yaml](./ansible/infrastructure/playbooks/templates/deploy-pg.yaml)                                   | Разворачивание StatefulSet БД PostgreSQL                             |
| [deployment.ansible.yaml](./ansible/infrastructure/playbooks/templates/deployment.ansible.yaml)                 | Deployment экземпляра Nginx                                          |
| [empty-dir.ansible.yaml](./ansible/infrastructure/playbooks/templates/empty-dir.ansible.yaml)                   | Pod экземпляра Nginx                                                 |
| [hosts.ansible.yaml.j2](./ansible/infrastructure/playbooks/templates/hosts.ansible.yaml.j2)                     | Шаблон параметров кластера                                           |
| [pg-pv.yaml](./ansible/infrastructure/playbooks/templates/pg-pv.yaml)                                           | PersistentVolume                                                     |
| [**Папка "playbooks/back-and-front"**](./ansible/infrastructure/playbooks/templates/back-and-front)             | Папка веб-приложения                                                 |
| [deploy-backend.yaml](./ansible/infrastructure/playbooks/templates/back-and-front/deploy-backend.yaml)          | Бэкенд приложения                                                    |
| [deploy-frontend.yaml](./ansible/infrastructure/playbooks/templates/back-and-front/deploy-frontend.yaml)        | Фронтенд приложения                                                  |
| [ingress.yaml](./ansible/infrastructure/playbooks/templates/back-and-front/ingress.yaml)                        | Ingress-правила                                                      |
| [**Папка "playbooks/debug"**](./ansible/infrastructure/playbooks/templates/debug)                               | Папка вспомогательных инструментов                                   |
| [dnsutils.ansible.yaml](./ansible/infrastructure/playbooks/templates/debug/dnsutils.ansible.yaml)               | Набор DNS-утилит                                                     |
| [minipg.ansible.yaml](./ansible/infrastructure/playbooks/templates/debug/minipg.ansible.yaml)                   | Клиент БД PostgreSQL                                                 |

Конфигурация доступа к кластеру сохранена в домашней папке пользователя по пути `~/.kube/config`:

![kube_folder.png](./images/kube_folder.png)

Теперь можно получить набор подов выполнив соответствующую команду `kubectl get pods`:

![kube_pods.png](./images/kube_pods.png)

На данный момент полная инфраструктура нашего Kubernetes-кластера выглядит следующим образом:

````bash
$ kubectl get all,cm,sts,svc,deploy,rs,po,pv,pvc,ep -A -o wide  
NAMESPACE              NAME                                            READY   STATUS    RESTARTS        AGE     IP               NODE    NOMINATED NODE   READINESS GATES
default                pod/nfs-server-nfs-server-provisioner-0         1/1     Running   3 (4m21s ago)   6h31m   10.200.104.10    node2   <none>           <none>
kube-system            pod/calico-kube-controllers-7f679c5d6f-bjbv7    1/1     Running   3 (4m21s ago)   6h35m   10.200.104.11    node2   <none>           <none>
kube-system            pod/calico-node-cj8bj                           1/1     Running   3 (4m21s ago)   6h36m   10.30.30.1       node2   <none>           <none>
kube-system            pod/calico-node-shddf                           1/1     Running   5 (4m16s ago)   6h36m   10.10.10.1       cp1     <none>           <none>
kube-system            pod/calico-node-z8qhq                           1/1     Running   3 (3m39s ago)   6h36m   10.20.20.1       node1   <none>           <none>
kube-system            pod/coredns-5867d9544c-7cjpn                    1/1     Running   3 (3m39s ago)   6h34m   10.200.166.136   node1   <none>           <none>
kube-system            pod/coredns-5867d9544c-dvs9n                    1/1     Running   3 (4m16s ago)   6h34m   10.200.176.8     cp1     <none>           <none>
kube-system            pod/dns-autoscaler-59b8867c86-xq559             1/1     Running   3 (4m16s ago)   6h34m   10.200.176.7     cp1     <none>           <none>
kube-system            pod/kube-apiserver-cp1                          1/1     Running   4 (4m16s ago)   6h38m   10.10.10.1       cp1     <none>           <none>
kube-system            pod/kube-controller-manager-cp1                 1/1     Running   4 (4m16s ago)   6h38m   10.10.10.1       cp1     <none>           <none>
kube-system            pod/kube-proxy-qbn5z                            1/1     Running   3 (3m39s ago)   6h36m   10.20.20.1       node1   <none>           <none>
kube-system            pod/kube-proxy-zr2t7                            1/1     Running   3 (4m16s ago)   6h36m   10.10.10.1       cp1     <none>           <none>
kube-system            pod/kube-proxy-zvlpb                            1/1     Running   3 (4m21s ago)   6h36m   10.30.30.1       node2   <none>           <none>
kube-system            pod/kube-scheduler-cp1                          1/1     Running   4 (4m16s ago)   6h38m   10.10.10.1       cp1     <none>           <none>
kube-system            pod/nginx-proxy-node1                           1/1     Running   3 (3m39s ago)   6h36m   10.20.20.1       node1   <none>           <none>
kube-system            pod/nginx-proxy-node2                           1/1     Running   3 (4m21s ago)   6h37m   10.30.30.1       node2   <none>           <none>
kube-system            pod/nodelocaldns-44fp6                          1/1     Running   3 (3m39s ago)   6h34m   10.20.20.1       node1   <none>           <none>
kube-system            pod/nodelocaldns-7snvl                          1/1     Running   3 (4m21s ago)   6h34m   10.30.30.1       node2   <none>           <none>
kube-system            pod/nodelocaldns-nzk5h                          1/1     Running   3 (4m16s ago)   6h34m   10.10.10.1       cp1     <none>           <none>
kubernetes-dashboard   pod/dashboard-metrics-scraper-8c47d4b5d-zg644   1/1     Running   3 (3m39s ago)   6h32m   10.200.166.135   node1   <none>           <none>
kubernetes-dashboard   pod/kubernetes-dashboard-6c75475678-b4mcn       1/1     Running   3 (4m21s ago)   6h32m   10.200.104.12    node2   <none>           <none>

NAMESPACE              NAME                                        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                                                                     AGE     SELECTOR
default                service/kubernetes                          ClusterIP   10.32.0.1       <none>        443/TCP                                                                                                     6h38m   <none>
default                service/nfs-server-nfs-server-provisioner   ClusterIP   10.32.138.154   <none>        2049/TCP,2049/UDP,32803/TCP,32803/UDP,20048/TCP,20048/UDP,875/TCP,875/UDP,111/TCP,111/UDP,662/TCP,662/UDP   6h31m   app=nfs-server-provisioner,release=nfs-server
kube-system            service/coredns                             ClusterIP   10.32.0.3       <none>        53/UDP,53/TCP,9153/TCP                                                                                      6h34m   k8s-app=kube-dns
kubernetes-dashboard   service/dashboard-metrics-scraper           ClusterIP   10.32.148.201   <none>        8000/TCP                                                                                                    6h32m   k8s-app=dashboard-metrics-scraper
kubernetes-dashboard   service/kubernetes-dashboard                ClusterIP   10.32.131.38    <none>        443/TCP                                                                                                     6h32m   k8s-app=kubernetes-dashboard

NAMESPACE     NAME                          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE     CONTAINERS    IMAGES                                          SELECTOR
kube-system   daemonset.apps/calico-node    3         3         3       3            3           kubernetes.io/os=linux   6h36m   calico-node   quay.io/calico/node:v3.24.5                     k8s-app=calico-node
kube-system   daemonset.apps/kube-proxy     3         3         3       3            3           kubernetes.io/os=linux   6h38m   kube-proxy    registry.k8s.io/kube-proxy:v1.24.4              k8s-app=kube-proxy
kube-system   daemonset.apps/nodelocaldns   3         3         3       3            3           kubernetes.io/os=linux   6h34m   node-cache    registry.k8s.io/dns/k8s-dns-node-cache:1.21.1   k8s-app=nodelocaldns

NAMESPACE              NAME                                        READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS                  IMAGES                                                            SELECTOR
kube-system            deployment.apps/calico-kube-controllers     1/1     1            1           6h35m   calico-kube-controllers     quay.io/calico/kube-controllers:v3.24.5                           k8s-app=calico-kube-controllers
kube-system            deployment.apps/coredns                     2/2     2            2           6h34m   coredns                     registry.k8s.io/coredns/coredns:v1.9.3                            k8s-app=kube-dns
kube-system            deployment.apps/dns-autoscaler              1/1     1            1           6h34m   autoscaler                  registry.k8s.io/cpa/cluster-proportional-autoscaler-amd64:1.8.5   k8s-app=dns-autoscaler
kubernetes-dashboard   deployment.apps/dashboard-metrics-scraper   1/1     1            1           6h32m   dashboard-metrics-scraper   kubernetesui/metrics-scraper:v1.0.8                               k8s-app=dashboard-metrics-scraper
kubernetes-dashboard   deployment.apps/kubernetes-dashboard        1/1     1            1           6h32m   kubernetes-dashboard        kubernetesui/dashboard:v2.6.1                                     k8s-app=kubernetes-dashboard

NAMESPACE              NAME                                                  DESIRED   CURRENT   READY   AGE     CONTAINERS                  IMAGES                                                            SELECTOR
kube-system            replicaset.apps/calico-kube-controllers-7f679c5d6f    1         1         1       6h35m   calico-kube-controllers     quay.io/calico/kube-controllers:v3.24.5                           k8s-app=calico-kube-controllers,pod-template-hash=7f679c5d6f
kube-system            replicaset.apps/coredns-5867d9544c                    2         2         2       6h34m   coredns                     registry.k8s.io/coredns/coredns:v1.9.3                            k8s-app=kube-dns,pod-template-hash=5867d9544c
kube-system            replicaset.apps/dns-autoscaler-59b8867c86             1         1         1       6h34m   autoscaler                  registry.k8s.io/cpa/cluster-proportional-autoscaler-amd64:1.8.5   k8s-app=dns-autoscaler,pod-template-hash=59b8867c86
kubernetes-dashboard   replicaset.apps/dashboard-metrics-scraper-8c47d4b5d   1         1         1       6h32m   dashboard-metrics-scraper   kubernetesui/metrics-scraper:v1.0.8                               k8s-app=dashboard-metrics-scraper,pod-template-hash=8c47d4b5d
kubernetes-dashboard   replicaset.apps/kubernetes-dashboard-6c75475678       1         1         1       6h32m   kubernetes-dashboard        kubernetesui/dashboard:v2.6.1                                     k8s-app=kubernetes-dashboard,pod-template-hash=6c75475678

NAMESPACE   NAME                                                 READY   AGE     CONTAINERS               IMAGES
default     statefulset.apps/nfs-server-nfs-server-provisioner   1/1     6h31m   nfs-server-provisioner   quay.io/kubernetes_incubator/nfs-provisioner:v2.3.0

NAMESPACE              NAME                                           DATA   AGE
default                configmap/kube-root-ca.crt                     1      6h37m
kube-node-lease        configmap/kube-root-ca.crt                     1      6h37m
kube-public            configmap/cluster-info                         5      6h38m
kube-public            configmap/kube-root-ca.crt                     1      6h37m
kube-system            configmap/calico-config                        2      6h36m
kube-system            configmap/coredns                              1      6h34m
kube-system            configmap/dns-autoscaler                       1      6h34m
kube-system            configmap/extension-apiserver-authentication   6      6h38m
kube-system            configmap/kube-proxy                           2      6h38m
kube-system            configmap/kube-root-ca.crt                     1      6h37m
kube-system            configmap/kubeadm-config                       1      6h38m
kube-system            configmap/kubelet-config                       1      6h38m
kube-system            configmap/kubernetes-services-endpoint         0      6h36m
kube-system            configmap/nodelocaldns                         1      6h34m
kubernetes-dashboard   configmap/kube-root-ca.crt                     1      6h32m
kubernetes-dashboard   configmap/kubernetes-dashboard-settings        0      6h32m

NAMESPACE              NAME                                                        ENDPOINTS                                                             AGE
default                endpoints/cluster.local-nfs-server-nfs-server-provisioner   <none>                                                                6h31m
default                endpoints/kubernetes                                        10.10.10.1:6443                                                       6h38m
default                endpoints/nfs-server-nfs-server-provisioner                 10.200.104.10:20048,10.200.104.10:662,10.200.104.10:111 + 9 more...   6h31m
kube-system            endpoints/coredns                                           10.200.166.136:53,10.200.176.8:53,10.200.166.136:53 + 3 more...       6h34m
kubernetes-dashboard   endpoints/dashboard-metrics-scraper                         10.200.166.135:8000                                                   6h32m
kubernetes-dashboard   endpoints/kubernetes-dashboard                              10.200.104.12:8443                                                    6h32m
````

Как результат, на трех виртуальных машинах Yandex.Cloud мы развернули работоспособный кластер Kubernetes и получили
конфигурацию доступа к нему в локальной папке `~/.kube`. Благодаря этому мы имеем возможность выполнения
команд `kubectl` из локального окружения.

---

## Этап 3. Создание тестового приложения

Подготовим простейшее [тестовое веб-приложение](./diploma-repo/web-app), состоящее из
единственной [html-страницы](./diploma-repo/web-app/index.html), отображающей некоторые сведения:

````html

<html>
<head>
    <meta http-equiv="Content-Type" content="text/html;charset=UTF-8">
    <title>DEVOPS-14</title>
    <link rel="icon" type="image/x-icon" href="favicon.png">
</head>
<body>
<h1>Diploma work DEVOPS-14</h1>
<h2>Zhuravlev O.E.</h2>
<h3>v.0.0.1</h3>
</body>
</html>
````

Также подготовим простую [конфигурацию веб-сервера](diploma-repo/nginx/nginx.conf), позволяющую выводить статические
данные (в нашем случае - страницу
[index.html](diploma-repo/web-app/index.html)):

````nginx configuration
server {
    listen 80;
    server_name  testref.com www.testref.com; # Name of virtual server (delivered in "HOST" header).

    access_log  /var/log/nginx/domains/testref.com-access.log  main; # Access logging.
    error_log   /var/log/nginx/domains/testref.com-error.log info;   # Error logging.

    location / { # Handles a certain type of client request.
        root /; #  Directory that will be used to search for a file
        index index.html;  # Defines files that will be used as an index.
        try_files $uri /index.html; # Checks whether the specified file or directory exists.
    }
}
````

Контейнеризируем данное веб-приложение с помощью Docker используя образ
веб-сервера [Nginx](https://hub.docker.com/_/nginx/):

````Dockerfile
FROM nginx
COPY web-app/index.html /usr/share/nginx/html
ADD nginx/nginx.conf /nginx/nginx.conf
````

В целях проверки работоспособности соберем образ:

````bash
$ docker build -t olezhuravlev/diploma:0.0.1 .
Sending build context to Docker daemon  56.32kB
Step 1/3 : FROM nginx
 ---> 2d389e545974
Step 2/3 : COPY web-app/index.html /usr/share/nginx/html
 ---> Using cache
 ---> 239ee511c3d7
Step 3/3 : ADD nginx/nginx.conf /nginx/nginx.conf
 ---> Using cache
 ---> 28f4b7049a10
Successfully built 28f4b7049a10
Successfully tagged olezhuravlev/diploma:0.0.1
````

Образ собран и присутствует в репозитории под тегом `olezhuravlev/diploma:0.0.1`:

````bash
$ docker image ls
REPOSITORY                TAG     IMAGE ID       CREATED          SIZE
olezhuravlev/diploma      0.0.1   28f4b7049a10   25 minutes ago   142MB
...
````

Запустим собранный образ локально:

````bash
$ docker run --network host olezhuravlev/diploma:0.0.1 
/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf
10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf
/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
/docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh
/docker-entrypoint.sh: Configuration complete; ready for start up
2023/01/17 15:42:14 [notice] 1#1: using the "epoll" event method
2023/01/17 15:42:14 [notice] 1#1: nginx/1.23.1
2023/01/17 15:42:14 [notice] 1#1: built by gcc 10.2.1 20210110 (Debian 10.2.1-6) 
2023/01/17 15:42:14 [notice] 1#1: OS: Linux 5.19.17-1-MANJARO
2023/01/17 15:42:14 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576
2023/01/17 15:42:14 [notice] 1#1: start worker processes
2023/01/17 15:42:14 [notice] 1#1: start worker process 31
2023/01/17 15:42:14 [notice] 1#1: start worker process 32
2023/01/17 15:42:14 [notice] 1#1: start worker process 33
2023/01/17 15:42:14 [notice] 1#1: start worker process 34
::1 - - [17/Jan/2023:15:42:18 +0000] "GET / HTTP/1.1" 304 0 "-" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36" "-"
````

Видим, что контейнеризированное веб-приложение доступно по `localhost` на стандартном порту `80`:

![webapp_v001.png](./images/webapp_v001.png)

Для хранения разворачиваемого веб-приложения создадим на [github.com](https://github.com) отдельный репозиторий
с именем ["diploma-repo"](https://github.com/olezhuravlev/diploma-repo):

![github_diploma_repo.png](./images/github_diploma_repo.png)

Для хранения же Docker-образа воспользуемся репозиторием [dockerhub.com](https://hub.docker.com/). Для этого
сначала залогинимся в него (учётная запись у нас уже имеется):

````bash
$ docker login                                                
Authenticating with existing credentials...
WARNING! Your password will be stored unencrypted in /home/oleg/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

Login Succeeded
````

И после успешной аутентификации отправим наш образ в репозиторий:

````bash
$ docker push olezhuravlev/diploma:0.0.1                   
The push refers to repository [docker.io/olezhuravlev/diploma]
7e46b64469bd: Pushed 
f1b7541425c2: Pushed 
36665e416ec8: Mounted from olezhuravlev/hello-world 
31192a8593ec: Mounted from olezhuravlev/hello-world 
7ee9bf58503c: Mounted from olezhuravlev/hello-world 
a064c1703bfd: Mounted from olezhuravlev/hello-world 
9388548487b1: Mounted from olezhuravlev/hello-world 
b45078e74ec9: Mounted from olezhuravlev/hello-world 
0.0.1: digest: sha256:a99757f9eb9cb6056aec64200fd35e721bf1130616357230da4ed9aa207b7486 size: 1984
````

После этого наш образ можно
[наблюдать в репозитории](https://hub.docker.com/repository/docker/olezhuravlev/diploma) через веб-интерфейс:

![dockerhub_diploma_image_v001.png](./images/dockerhub_diploma_image_v001.png)


Таким образом мы создали [git-репозиторий](https://github.com/olezhuravlev/diploma-repo),
содержащий [тестовое веб-приложение](https://github.com/olezhuravlev/diploma-repo/tree/main/web-app) и
[Dockerfile](https://github.com/olezhuravlev/diploma-repo/blob/main/Dockerfile) для его контейнеризации, а также
разместили в реестре Dockerhub
[контейнеризированный образ](https://hub.docker.com/repository/docker/olezhuravlev/diploma) этого веб-приложения.

---

## Этап 4. Подготовка cистемы мониторинга и разворачивание приложения в кластере Kubernetes


---
